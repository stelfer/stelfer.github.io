<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>memoize.me</title><link href="/" rel="alternate"></link><link href="/feeds/atom.xml" rel="self"></link><id>/</id><updated>2018-11-01T00:00:00-07:00</updated><entry><title>What's coming next...</title><link href="/2018-11-01-next.html" rel="alternate"></link><published>2018-11-01T00:00:00-07:00</published><updated>2018-11-01T00:00:00-07:00</updated><author><name>Soren Telfer</name></author><id>tag:None,2018-11-01:/2018-11-01-next.html</id><summary type="html">&lt;p&gt;I'm going to start doing this regularly in 2019. I'm making this post now to give myself a bit of
&lt;span class="math"&gt;\(v_{0}\)&lt;/span&gt; going into the new year. A short list of what's coming:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A while back I was messing with &lt;a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle"&gt;n-queens&lt;/a&gt;, and I found myself needing to test board
permutations against the internal symmetries of chess. That lead me back to one of my favorite
things to think about, generating permutations. This lead me to looking at
&lt;a href="https://en.wikipedia.org/wiki/Cayley_graph"&gt;Cayley graphs&lt;/a&gt; of &lt;span class="math"&gt;\(D_4\)&lt;/span&gt;, which lead me to &lt;a href="https://arxiv.org/abs/1203.1835"&gt;bell ringing&lt;/a&gt; from the 17th
century . I'll explain.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recently I found myself reading about &lt;a href="https://en.wikipedia.org/wiki/Belief_propagation"&gt;belief …&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;I'm going to start doing this regularly in 2019. I'm making this post now to give myself a bit of
&lt;span class="math"&gt;\(v_{0}\)&lt;/span&gt; going into the new year. A short list of what's coming:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A while back I was messing with &lt;a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle"&gt;n-queens&lt;/a&gt;, and I found myself needing to test board
permutations against the internal symmetries of chess. That lead me back to one of my favorite
things to think about, generating permutations. This lead me to looking at
&lt;a href="https://en.wikipedia.org/wiki/Cayley_graph"&gt;Cayley graphs&lt;/a&gt; of &lt;span class="math"&gt;\(D_4\)&lt;/span&gt;, which lead me to &lt;a href="https://arxiv.org/abs/1203.1835"&gt;bell ringing&lt;/a&gt; from the 17th
century . I'll explain.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recently I found myself reading about &lt;a href="https://en.wikipedia.org/wiki/Belief_propagation"&gt;belief propagation&lt;/a&gt;. This lead me to a
&lt;a href="https://arxiv.org/abs/1702.00467"&gt;2017 paper&lt;/a&gt; from &lt;a href="http://tuvalu.santafe.edu/~moore/"&gt;Chris Moore&lt;/a&gt;, talking about the hardness of certain graph
problems in the setting of statistical physics and random graph theory. It lead me to thinking about
why Facebook needs trolls. I'm sure this is well trodden ground, but I'll explain my thoughts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I want to write up how I've learned to think about weather, particularly how I think about the
expected utility of purchasing a ski pass for Tahoe.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I recently read on HackerNews the assertion that "Nakamoto Consensus" was a "CS breakthrough" in
the &lt;a href="https://en.wikipedia.org/wiki/Byzantine_fault_tolerance#Byzantine_Generals'_Problem"&gt;Byzantine Generals Problem&lt;/a&gt;. That seems to me a bit strong, but I want to
explore in 2018 how much of that statement is true. I'm also interested how many young engineers out
there share this view.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the things we think about (and obliquely work on), is the role of Machine Learning in
entertainment and content creation. There's a narrative out there that Netflix told the creators of
House of Cards to make a show with Kevin Spacey in it because they knew it would be a hit.  That's
not &lt;a href="https://www.wired.com/2012/11/netflix-data-gamble/"&gt;actually what happend&lt;/a&gt;.  I'm interested in the history of this myth, vis-a-vis
the rise of Big Data, and the AI, as SV hype tribes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ultimately, I need to write about innovation. I've been running an innovation center for a $150B+
company for going on six years now. I have a lot of thoughts to share about innovation programs,
innovation in general, and the future of R&amp;amp;D.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "2em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Planning"></category><category term="Motivation"></category></entry><entry><title>A Different View on the Coupon Collector Problem</title><link href="/coupon-collector.html" rel="alternate"></link><published>2018-10-27T00:00:00-07:00</published><updated>2018-10-27T00:00:00-07:00</updated><author><name>Soren Telfer</name></author><id>tag:None,2018-10-27:/coupon-collector.html</id><summary type="html">&lt;p&gt;Once during a meeting in which we were discussing the &lt;a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle"&gt;Fischer-Yates shuffle&lt;/a&gt; someone
asked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How many times do we need to run the algorithm to see every permutation?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a really great question and it caught me off guard. The reason was that since Fischer-Yates
is a random algorithm, the answer really involves thinking about random processes. So, like, I can't
really answer that question as it is posed.  While thinking aloud as I struggled to answer, I
reduced the problem to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What's the average number of times I need to flip a coin in order to see both …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;Once during a meeting in which we were discussing the &lt;a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle"&gt;Fischer-Yates shuffle&lt;/a&gt; someone
asked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How many times do we need to run the algorithm to see every permutation?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a really great question and it caught me off guard. The reason was that since Fischer-Yates
is a random algorithm, the answer really involves thinking about random processes. So, like, I can't
really answer that question as it is posed.  While thinking aloud as I struggled to answer, I
reduced the problem to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What's the average number of times I need to flip a coin in order to see both sides?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Enter The Coupon Collector&lt;/h3&gt;
&lt;p&gt;I know it says coupons, and I was just mentioning a coin, but let's talk about
&lt;a href="https://en.wikipedia.org/wiki/Happy_Meal"&gt;Happy Meals&lt;/a&gt;.  Say you were a child in the 80's and McDonalds started running one of
their happy meal promotions. They create a number (usually &lt;span class="math"&gt;\(\approx 10\)&lt;/span&gt;) of plastic trinkets that
you get one of every time you buy a happy meal. You, as the child, love them. Your parents, as the
ones who have to drive you to McDonalds, are wondering how many more times they need to drive you to
McDonalds.  It turns out than when you purchase a Happy Meal, instead of letting you pick which
trinket your child wants, they just randomly give you whichever one comes out of the bin of trinkets
they have under the counter.  In an earlier, more idyllic America, people apparently had to do this
with coupons in the newspaper.&lt;/p&gt;
&lt;p&gt;Regardless of how you want to think about it, let's say that there are &lt;span class="math"&gt;\(m\)&lt;/span&gt; trinkets, to which you
assign an arbitrary order and index &lt;span class="math"&gt;\(1 \leq i \leq m\)&lt;/span&gt;. The classic way of getting the result is
provided by &lt;a href="https://en.wikipedia.org/wiki/Coupon_collector%27s_problem"&gt;Wikipedia&lt;/a&gt;, which asks you to think about &lt;span class="math"&gt;\(t_i\)&lt;/span&gt;, the number of trials to get
the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th trinket after having completed the labor of obtaining the previous &lt;span class="math"&gt;\(i - 1\)&lt;/span&gt; trinkets.
What's the probability of getting the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th trinket? Well it's &lt;span class="math"&gt;\(p_i = (m - (i-1))/m\)&lt;/span&gt;. Then the
expected number of trials to get the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th is &lt;span class="math"&gt;\(1/p_i\)&lt;/span&gt;, just like the expected number of times to
roll a 1 on a six-sided die is 6. The total number of trials, then, is calculated by adding up all
of these individual exercises in obtaining trinkets. Let's call this total &lt;span class="math"&gt;\(T\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[T] &amp;amp;= \sum_{i=1}^m E[t_i] = \sum_{i=1}^m \frac{1}{p_i}\\\
 &amp;amp;= \frac{m}{m} + \frac{m}{m-1} + ... + \frac{m}{1}\\\
 &amp;amp;= m\left(1 + \frac{1}{2} + ... + \frac{1}{m}\right) = m H_m\\\
\end{align}&lt;/div&gt;
&lt;p&gt;I'll be honest and say that I &lt;em&gt;really&lt;/em&gt; don't like this derivation. It seems like a trick. This
certainly isn't the way I think about sums of random variables. But this is way most people are
taught to solve this problem. In any event, I didn't answer the original question about
Fischer-Yates like this.  The rest of this post will take you on a seemingly circuitous path towards
something that is in the end very simple, and very beautiful.  I think it also proves that when all
else fails, brute force counting can take you a long way, provided you have the machinery to get you
there.&lt;/p&gt;
&lt;h3&gt;Flipping a coin to see both sides&lt;/h3&gt;
&lt;p&gt;So, let's get back to the coin. We can model the situation as a sequence of experiments in flipping
them.  These experiments generate sequences in which the &lt;span class="math"&gt;\(n\)&lt;/span&gt;-th event is &lt;span class="math"&gt;\(H\)&lt;/span&gt; or &lt;span class="math"&gt;\(T\)&lt;/span&gt; and the &lt;span class="math"&gt;\(n-1\)&lt;/span&gt;
other events are all either &lt;span class="math"&gt;\(T\)&lt;/span&gt; or &lt;span class="math"&gt;\(H\)&lt;/span&gt; respectively. What would these sequences look like?  If we
start enumerating the possibilities, we can see patterns appear, depending on how long we run the
experiments.  For the sequences of length at most &lt;span class="math"&gt;\(n\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_n = &amp;amp;HT + TH + \\
    &amp;amp;HHT + TTH + \ldots \\
    &amp;amp;H^{n-1}T + T^{n-1}H.\\
\end{align}&lt;/div&gt;
&lt;p&gt;It's interesting that the generation of length &lt;span class="math"&gt;\(k\)&lt;/span&gt; is easy to &lt;em&gt;generate&lt;/em&gt; from the &lt;span class="math"&gt;\((k-1)\)&lt;/span&gt;-th by
simply prepending the opposite of the terminal token, which doesn't change.  The production rule
here is quite simple.  Anyway, what do we do with this sum? Well, if we think about &lt;span class="math"&gt;\(H\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt; as
probabilities &lt;span class="math"&gt;\(P(H) = p\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(T) = q = 1 - p\)&lt;/span&gt;, 
&lt;span class="math"&gt;\(S_\infty\)&lt;/span&gt; better sum to &lt;span class="math"&gt;\(1\)&lt;/span&gt;, so we can evaluate by
collecting terms:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_{\infty} &amp;amp;= T\left(H + H^2 + H^3 + \ldots\right) + H\left(T + T^2 + T^3 + \ldots\right)\\\
&amp;amp;= T\left(\frac{1}{1-H} - 1\right) + H\left(\frac{1}{1-T} - 1\right)\\\
&amp;amp;= HT\left(\frac{1}{1-H} + \frac{1}{1-T}\right)\label{S}\\
&amp;amp;= pq \left(\frac{p+q}{pq}\right) = 1.
\end{align}&lt;/div&gt;
&lt;p&gt;Now to calculate &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt; we interpret &lt;span class="math"&gt;\(S_k\)&lt;/span&gt; as a probability density, and evaluate the sum:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[n] &amp;amp;= \sum_{n} n S_n\\
&amp;amp;=\,2(pq + qp)\, + 3(ppq + qqp) + \ldots m(p^{m-1}q + q^{m-1}p) \ldots...
\end{align}&lt;/div&gt;
&lt;p&gt;Regrouping a bit, we see that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[n] &amp;amp;= q\left(2p + 3p^2 + \ldots mp^{m-1} + \ldots\right) + p\left(2q + 3p^2 + \ldots mq^{m-1} + \ldots \right) \\\
    &amp;amp;= \frac{q}{p} \left(\sum_{i=1}^\infty ip^i - p\right) + \frac{p}{q} \left(\sum_{i=1}^\infty iq^i - q\right)\\\
&amp;amp;= \frac{q}{p}\left(\frac{p}{q^2} - p\right) + \frac{p}{q}\left(\frac{q}{p^2} - q\right)\\\
&amp;amp;= \frac{p}{q}\left(\frac{1}{1-q} - q\right) + \frac{q}{p}\left(\frac{1}{1-p} - p\right)\\\
&amp;amp;= \frac{1 - pq}{p(1-p)}\\\
&amp;amp;= \frac{p^2 - p + 1}{p(1-p)}\label{old_way}\\
&amp;amp;= 2 + \frac{p^3 + q^3}{pq}\\\
\end{align}&lt;/div&gt;
&lt;p&gt;This was new to me. For &lt;span class="math"&gt;\(p = q\)&lt;/span&gt; we have that &lt;span class="math"&gt;\(E[n] = 3\)&lt;/span&gt;. The average number of times you need to
flip a fair coin to see both sides is 3. If &lt;span class="math"&gt;\(p = 0.25\)&lt;/span&gt;, then &lt;span class="math"&gt;\(E[n] = 4.\bar{33}\)&lt;/span&gt;. If the coin is only
10% fair, then &lt;span class="math"&gt;\(E[n] \approx 10\)&lt;/span&gt;. If you're interested in how it varies with &lt;span class="math"&gt;\(p\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="images/fair-coin-expectation.png"/&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;This is maybe a way of estimating if a coin is &lt;em&gt;extremely&lt;/em&gt; unfair: count the number flips it takes
to see both sides.&lt;/p&gt;
&lt;h3&gt;Fine for coins, what about the trinkets?&lt;/h3&gt;
&lt;p&gt;Having solved the problem for &lt;span class="math"&gt;\(m = 2\)&lt;/span&gt;, the next thing to do was to try for higher &lt;span class="math"&gt;\(m\)&lt;/span&gt;'s.  If I were running a Happy Meal campaign, I might think about how to manipulate the &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; in order to maximize &lt;span class="math"&gt;\(E[T]\)&lt;/span&gt;.  Some options here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Naive approach: all equal probabilities, i.e. &lt;span class="math"&gt;\( p_k = 1/m\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Singularly hard to get: One distinctive &lt;span class="math"&gt;\(p_j\)&lt;/span&gt;, but all other &lt;span class="math"&gt;\(p\)&lt;/span&gt;'s equal&lt;/li&gt;
&lt;li&gt;Anything goes: arbitrary &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; subject to &lt;span class="math"&gt;\(\sum p_i = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think if you grew up in the 80's you might have felt like McDonalds used the second approach.  But
it turns out that the first is a lot easier to analyze, and will carry us through to the thing I
want to show in this post.&lt;/p&gt;
&lt;p&gt;The first thing that happened when we tried to do brute-force counting for &lt;span class="math"&gt;\(m \gt 2\)&lt;/span&gt;, was that the
sequences quickly got out of hand. We needed a better approach. You may have noticed that I used the
word &lt;em&gt;generate&lt;/em&gt; above. That was a hint. &lt;a href="https://en.wikipedia.org/wiki/Generating_function"&gt;Generating Functions&lt;/a&gt; are an incredibly useful
tool in the toolbox of the brute-forcing amateur combinatorialist community. They're also pretty
fear-inspiring to a lot of people. For whatever reason, when I got tired of brute force counting the
sequences for &lt;span class="math"&gt;\(m = 3\)&lt;/span&gt;, I remembered some advice from &lt;a href="http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025"&gt;&lt;em&gt;Knuth et. al.&lt;/em&gt;&lt;/a&gt; to use finite
automata to keep mayhem from breaking out in the production rules.  But before we get to that, it's
first useful to introduce the machinery that we will use when we get there.&lt;/p&gt;
&lt;h3&gt;The coin redux, this time as a PGF&lt;/h3&gt;
&lt;p&gt;I'm not going to do justice to generating functions in this space. I reccomend &lt;em&gt;Knuth et. al.&lt;/em&gt; as a
gentle introduction and &lt;a href="https://www.amazon.com/generatingfunctionology-Third-Herbert-S-Wilf/dp/1568812795/"&gt;&lt;em&gt;Wilf&lt;/em&gt;&lt;/a&gt; if you're into that kind of thing.  But I will give the basic
ideas.  Imagine that you have a function &lt;span class="math"&gt;\(G\)&lt;/span&gt; that you are able to expand in a (complex) power series
that converges in a sense that it is simply not silly to act like it does:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \sum_{k=0}^{\infty} c_k z^n \lt \infty\label{gf_def}\\ 
\end{align}&lt;/div&gt;
&lt;p&gt;Suppose you also happen to have a recurrence relation for &lt;span class="math"&gt;\(c_j = f(\left\{c_i : i &amp;lt; j\right\})\)&lt;/span&gt;.  There is an
arsenal not unlike that available for Fourier or Laplace transforms that allow you to derive from
that recurrence an analytic function &lt;span class="math"&gt;\(G(z)\)&lt;/span&gt;, which can be expanded into the form above.  Then, a
&lt;em&gt;closed-form&lt;/em&gt; version of &lt;span class="math"&gt;\(c_j\)&lt;/span&gt; can read off from the &lt;span class="math"&gt;\(c_k\)&lt;/span&gt;'s in the series expansion of &lt;span class="math"&gt;\(G(z)\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;If this is new to you, I'll say that it really helps to see it in action. But, we're not going to do
any of that here.  Instead, we are going to take some liberties and just use the machinery.  To
whit, the first thing we are going to do is realize that any series sum that involves powers of
things can be "converted" to a GF by taking some liberties and making some suitable replacements.&lt;/p&gt;
&lt;p&gt;Why would we want to do this?  Well, it all depends on how you choose to interpret the coefficients.
In particular, if you interpret the &lt;span class="math"&gt;\(c_k\)&lt;/span&gt; as &lt;em&gt;probabilities&lt;/em&gt;, then you actually get something that
is formally known as a probability generating function (PGF).&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \sum_{k=0}^{\infty} p_k z^k\\\
\end{align}&lt;/div&gt;
&lt;p&gt;Notice a couple of things here: First, that &lt;span class="math"&gt;\(G(1) = 1\)&lt;/span&gt;.  Why? well 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(1) &amp;amp;= \sum_{k=0}^{\infty} p_k 1^k\\\
    &amp;amp;= \sum_{k=1}^{\infty} p_k = 1\\\
\end{align}&lt;/div&gt;
&lt;p&gt;for everything that we consider to be a probability.  Also, notice that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G'(z) &amp;amp;= \sum_{k=1}^{\infty} k p_k z^{k-1}\\\
&amp;amp;\implies G'(1) = \sum_{k=1}^{\infty} k p_k = E[n].\\\
\end{align}&lt;/div&gt;
&lt;p&gt;Now this is useful to us. Brute-force counting sequences to calculate &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt; could be replaced by
calculating the derivitive of the PGF for the process.  If only we knew how to create the
PGF! Before I show you, let's observe one more fact about PGFs. Suppose we find ourselves in a
situation where we have a PGF that looks something like this&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \frac{P(z)}{F(z)}\,\\
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(P(z)\)&lt;/span&gt; and &lt;span class="math"&gt;\(F(z)\)&lt;/span&gt; are some analytic functions of &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Observe that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
&amp;amp;F'(z)\,G(z) + G'(z)\,F(z) = P'(z)\\\
&amp;amp;\implies G'(1) = \frac{P'(1) - F'(1)}{F(1)}\\
\end{align}&lt;/div&gt;
&lt;p&gt;If you have ever spent any time with partial fractions, you will know why we will want this result.
Now, let's use all of this on an unsuspecting sum to see if it works.  In light of thinking about
PGFs, our original &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(T\)&lt;/span&gt; and &lt;span class="math"&gt;\(H\)&lt;/span&gt; is worth talking a look at.So, lets make the
replacements &lt;span class="math"&gt;\(H \rightarrow pz\)&lt;/span&gt; and &lt;span class="math"&gt;\(T \rightarrow qz\)&lt;/span&gt; and see what happens. This will allow us to
promote our proto-PGF &lt;span class="math"&gt;\(S \rightarrow G\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G &amp;amp;= p q z^2 \left(\frac{1}{1-pz} + \frac{1}{1-qz}\right) = \frac{P(z)}{F(z)}\\\
&amp;amp;= \frac{p q z^2 \left(2-z\right)}{(1-pz)(1-qz)}\\\
P'(1)&amp;amp;= p(1-p)\\\
F'(1)&amp;amp;= 2p - 2p^2 -1\\\
G'(1)&amp;amp;= \frac{p(1-p) - (2p -2p^2 - 1)}{p(1-p)}\\\
&amp;amp;= \frac{p^2 - p + 1}{p(1-p)}.
\end{align}&lt;/div&gt;
&lt;p&gt;If you look back at our earlier calculation of &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt;, you'll see the same exact result.  So, we
accomplished nothing, except to convince ourselves that our machinery works.&lt;/p&gt;
&lt;h3&gt;The coin again, this time as a finite automaton&lt;/h3&gt;
&lt;p&gt;We return now to the hint that &lt;em&gt;Knuth et. al.&lt;/em&gt; gave us about using finite automata to help generate
our &lt;span class="math"&gt;\(S\)&lt;/span&gt;'s.  If we think about the coin problem we've been considering, a first stab at at
state machine could look something like this.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img alt="Alt Text" src="images/coin-automata.svg"&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;I've labeled the states by number as well. We start in a state &lt;span class="math"&gt;\(0\)&lt;/span&gt; where we haven't seen a flip
yet. Then we start accumulating flips, moving to &lt;span class="math"&gt;\(1\)&lt;/span&gt; or &lt;span class="math"&gt;\(3\)&lt;/span&gt; initially after a &lt;span class="math"&gt;\(T\)&lt;/span&gt; or &lt;span class="math"&gt;\(H\)&lt;/span&gt;, then
eventually terminating when we get a flip different from the first flip we saw.  Now comes the
interesting part. Let's write down the sequence sum, like we did before, this time using the
subscript on &lt;span class="math"&gt;\(S\)&lt;/span&gt; not to index the number of trials, but to index the state &lt;span class="math"&gt;\(S_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_0 &amp;amp;= 1\\\
S_1 &amp;amp;= TS_0 + TS_1 = T(1+S_1)\\\
S_2 &amp;amp;= HS1\\\
S_3 &amp;amp;= H(1+S3)\\\
S_4 &amp;amp;= TS_3\\\
S &amp;amp;= S_2 + S_4.\\
\end{align}&lt;/div&gt;
&lt;p&gt;Here we have identified &lt;span class="math"&gt;\(S_2\)&lt;/span&gt; and &lt;span class="math"&gt;\(S_4\)&lt;/span&gt; as our terminal states. Solving we see that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_2 &amp;amp;= \frac{TH}{1-T}\\\
S_4 &amp;amp;= \frac{TH}{1-H}\\\
S &amp;amp; = TH \left(\frac{1}{1-H} + \frac{1}{1-T}\right)\\
\end{align}&lt;/div&gt;
&lt;p&gt;This form should be familiar enough now that I don't have to convince you that this approach worked.&lt;/p&gt;
&lt;h3&gt;Happy Meals with Equal Probabilities&lt;/h3&gt;
&lt;p&gt;We are now prepared to take on the CC problem for arbitrary &lt;span class="math"&gt;\(m\)&lt;/span&gt; with all &lt;span class="math"&gt;\(p_i = 1/m\)&lt;/span&gt;.  As before,
define a finite automata to describe the process.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="/images/m-automata.svg"&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Here the states are labeled by the number of distinct trinkets we've seen. The symbol &lt;span class="math"&gt;\(X\)&lt;/span&gt; takes
the place of the &lt;span class="math"&gt;\(H\)&lt;/span&gt; or &lt;span class="math"&gt;\(T\)&lt;/span&gt; and represents the event of seeing a trinket. At the start, there are
&lt;span class="math"&gt;\(m\)&lt;/span&gt; ways we can move to state &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Once we are in state &lt;span class="math"&gt;\(1\)&lt;/span&gt;, there is one way we stay in that state,
and &lt;span class="math"&gt;\((m-1)\)&lt;/span&gt; ways we can move to state &lt;span class="math"&gt;\(2\)&lt;/span&gt;. And so on.&lt;/p&gt;
&lt;p&gt;Then, as before we can read off the proto-PGF&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_0 &amp;amp;= 1\\\
S_1 &amp;amp;= m X S_0 + X S_1 = \frac{m}{1-X}\,XS_0\\\
S_2 &amp;amp;= \frac{(m - 1)}{1 - 2 X}\,XS_0\\\
S_k &amp;amp;= \frac{(m - k + 1)}{1 - k X}\,XS_{k - 1}\\\
&amp;amp;= f(X,m)_k\,XS_{k-1}\\\
S_m &amp;amp;= XS_{m-1}.\\\
\end{align}&lt;/div&gt;
&lt;p&gt;Writing down a few terms for small k of the recurrence let's us see a closed form&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_4 &amp;amp;= X S_3 \\\
    &amp;amp;= X^2 f(3)\,S_2 \\\
    &amp;amp;= X^3 f(3)\,f(2)\,S_1 \\\
    &amp;amp;= X^4 \prod_{k=1}^{3} f(k). \\\
\end{align}&lt;/div&gt;
&lt;p&gt;This leads us to conclude that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_m &amp;amp;= X^m \prod_{k=1}^{m-1} \frac{m - k + 1}{1 - k X}\\\
    &amp;amp;= m! X^m \prod_{k=1}^{m-1} \left(1 - k X \right)^{-1}.\\\
\end{align}&lt;/div&gt;
&lt;p&gt;As before, we make the substitution &lt;span class="math"&gt;\( X \rightarrow pz = z/m\)&lt;/span&gt;, and we promote the proto-&lt;span class="math"&gt;\(PGF\)&lt;/span&gt; to a &lt;span class="math"&gt;\(PGF\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \frac{m!}{m^m} z^m \prod_{k=1}^{m-1} (1-kz/m)^{-1}.\\\
\end{align}&lt;/div&gt;
&lt;p&gt;Also, as before, we make the substitution &lt;span class="math"&gt;\(G=z^{m}/F\)&lt;/span&gt; and recall that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[n] = G'(1) = \frac{m - F'(1)}{F(1)} \label{mean}\\
\end{align}&lt;/div&gt;
&lt;p&gt;Then
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
F(z) &amp;amp;= \frac{m^m}{m!} \prod_{k=1}^{m-1} (1- kz/m) = \frac{m^m}{m!} Q_m(z)\\\
F'(z) &amp;amp;= \frac{-m^m}{m!} \frac{z}{m} Q_m(z) \sum_{k=1}^{m-1} \left(\frac{k}{1-kz/m}\right) \\\
      &amp;amp;= -\frac{z}{m} F(z) \sum_{k=1}^{m-1} \left(\frac{k}{1-kz/m}\right) \\\
      &amp;amp;= -z F(z) \sum_{k=1}^{m-1} \left(\frac{k}{m - kz}\right) \\\
      &amp;amp;= z F(z) \sum_{k=1}^{m-1} \left(\frac{1}{z - m/k}\right) \label{Fprime}\\
\end{align}&lt;/div&gt;
&lt;p&gt;Things are looking very promising.  Now we evaluate &lt;span class="math"&gt;\(Q(1)\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
Q(1) &amp;amp;= \frac{1}{m^{m-1}} \prod_{k=1}^{m-1} \left(m-k\right) \ \\
     &amp;amp;= \frac{1}{m^{m-1}} \left( (m-1)(m-2) \ldots \left( m - (m - 1) \right) \right)\ \\
     &amp;amp;= \frac{\left(m-1\right)!}{m^{m-1}}\\\
     &amp;amp;= \frac{m!}{m^m}.\\\
\end{align}&lt;/div&gt;
&lt;p&gt;Interestingly, this means that &lt;span class="math"&gt;\(F(1) =1\)&lt;/span&gt;, which also means that &lt;span class="math"&gt;\(F\)&lt;/span&gt; also behaves like a PGF.  At any
rate, we were now left with&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
F'(1) &amp;amp;= \sum_{k=1}^{m-1} \frac{1}{1 - m/k}\\\
\end{align}&lt;/div&gt;
&lt;p&gt;By now, we are tired and we resort to some &lt;a href="http://www.cs.cmu.edu/~odonnell/toolkit13/how-to-do-math-and-tcs.pdf"&gt;street fighting&lt;/a&gt;, &lt;a href="https://www.wolframalpha.com/input/?i=sum+1%2F(1-m%2Fk),+k%3D1+to+m-1"&gt;Wolfram Alpha&lt;/a&gt;-style, which gives us
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
F'(1) &amp;amp;= m \left( 1 - \psi^{(0)} - \gamma\right) - 1\\\
      &amp;amp;= m \left( 1 - (H_{m-1} - \gamma) - \gamma \right) - 1\\\
      &amp;amp;= m \left( 1 - H_{m-1} \right) - 1\\\
\end{align}&lt;/div&gt;
&lt;p&gt;using the fact that &lt;span class="math"&gt;\(\psi^{(0)} = H_{m-1} - \gamma\)&lt;/span&gt;. Of course, this fact is exactly what Alpha used
to give us the answer by re-arranging the sum.  Plugging this all in, we finally get&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G'(1) &amp;amp;= E[n]\\\
      &amp;amp;= m - \left(m - m H_{m-1} - 1 \right)\\\
      &amp;amp;= 1 + m H_{m-1}\\\
      &amp;amp;= mH_m\\
\end{align}&lt;/div&gt;
&lt;p&gt;using &lt;span class="math"&gt;\(H_m = H_{m-1} + 1/m\)&lt;/span&gt;. This was our goal. &lt;/p&gt;
&lt;p&gt;Now to answer the original question for the equally probably Happy Meals.  In general we can get a
one-sided confidence interval for &lt;span class="math"&gt;\(n\)&lt;/span&gt;, the number of times we "roll" until success, by using &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt;
and Markov's inequality.  If we could be bothered to calculate the variance using the formula's
above, we could use Chebyshev's to get a two sided confidence interval and a tighter bound. Anyway,&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
P(n &amp;gt; k m H_m) &amp;amp;\leq \frac{E[n]}{k m H_m}\\\
&amp;amp;\leq \frac{1}{k}.\\\
\end{align}&lt;/div&gt;
&lt;p&gt;Setting &lt;span class="math"&gt;\(1/k = 0.05 \implies k = 20\)&lt;/span&gt;. For example, for a six-sided die, for &lt;span class="math"&gt;\(\epsilon = kmH_m\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
P(n &amp;gt; \epsilon) &amp;amp;\leq 0.05\\\
\epsilon &amp;amp;\approx 300 
\end{align}&lt;/div&gt;
&lt;p&gt;For a permutation of order &lt;span class="math"&gt;\(6\)&lt;/span&gt;, the 95% threshold is &lt;span class="math"&gt;\(\approx 200000\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, for large &lt;span class="math"&gt;\(m\)&lt;/span&gt;, &lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
H_m &amp;amp;\approx \ln{m} + \gamma\\\
E[n] &amp;amp;\approx m\left( \ln{m} + \gamma \right)\\\
\end{align}&lt;/div&gt;
&lt;p&gt;In my weird and twisted way, this is a much more satisfying approach to look at the Coupon
Collector. Along the way, we practiced a technique that seems to be very powerful. For instance, I'm
interested to try my hand at using this technique on the &lt;a href="https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm"&gt;KMP Algorithm&lt;/a&gt;. I'm a fan of
&lt;a href="https://www.ics.uci.edu/~eppstein/"&gt;David Eppstein's Notes&lt;/a&gt; on algorithms from &lt;a href="https://www.ics.uci.edu/~eppstein/161/syl.html"&gt;long ago in internet time&lt;/a&gt;,
particularly where he shows a fsm for KMP:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;
&lt;img src="https://www.ics.uci.edu/~eppstein/161/kmp.gif"/&gt;
&lt;/center&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "2em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Generating Functions"></category><category term="Probability"></category><category term="Combinatorics"></category></entry><entry><title>Why you should know about the Gamma distribution</title><link href="/the-gamma-distribution.html" rel="alternate"></link><published>2016-12-02T00:00:00-08:00</published><updated>2016-12-02T00:00:00-08:00</updated><author><name>Soren Telfer</name></author><id>tag:None,2016-12-02:/the-gamma-distribution.html</id><summary type="html">&lt;p&gt;I've been staring at exponentially distributed data on a regular basis in one form or another for at
least the last ten years.  In my previous job, I spent a lot of time with the statistics of VoIP
phone calls, trying to understand what's "normal", and what are symptoms of system pathologies, or
even the actions of fraudsters in the telephony networks.  In approaching this subject, I had never
yet studied queuing theory, but I had a lot of experience with Poisson processes in experimental
high energy physics. This meant that as I learned I had many wonderful eureka! moments …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been staring at exponentially distributed data on a regular basis in one form or another for at
least the last ten years.  In my previous job, I spent a lot of time with the statistics of VoIP
phone calls, trying to understand what's "normal", and what are symptoms of system pathologies, or
even the actions of fraudsters in the telephony networks.  In approaching this subject, I had never
yet studied queuing theory, but I had a lot of experience with Poisson processes in experimental
high energy physics. This meant that as I learned I had many wonderful eureka! moments.  One of
these moments was when I learned that the sum of exponentially distributed variables has a gamma
distribution. I often find that people I mention this to, people in systems, and even often in
machine learning, are unfamiliar with this fact, or why it would be useful to know.&lt;/p&gt;
&lt;p&gt;In fact, this is a very helpful thing to know when looking at the distributions of many things you
measure in systems, for you often see a long-tailed distribution where the mode is shifted to the
right, relative to where the exponential would start. This is what &lt;code&gt;scipy.stats.gamma.rvs(1.99,
size=1000)&lt;/code&gt; looks like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="A Gamma" src="/images/gamma.png"&gt;&lt;/p&gt;
&lt;p&gt;The reason this is really helpful to think about is you often find yourself looking at the length of
the intervals it takes for things to complete: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the time it takes IO requests to complete&lt;/li&gt;
&lt;li&gt;the time it takes a program to run&lt;/li&gt;
&lt;li&gt;the amount of time a lock is held&lt;/li&gt;
&lt;li&gt;the time it takes to resolve a ticket&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One way to think about what is happening behind the scenes is that you are looking at many
different system components taking part, and each taking some time to do it's job. What you see then
when you wait for the entire completion is then the sum of many individual component times. When we
model the individual times as exponentially distributed, we then should expect that what we measure
should follow a gamma-like distribution.  When reasoning about systems, looking at the distribution
of times tells you a lot about what's happening.  When the data looks exponentially-distributed,
i.e. the mode is all the way to the left (&lt;code&gt;scipy.stats.expon.rvs(size=1000)&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;img alt="An exponential" src="/images/expon.png"&gt;&lt;/p&gt;
&lt;p&gt;then you really have one dominant process contributing to the completion time. Instead, the more you see the mode shifted to the right, the more you can reason that there are a number of processes of equal contribution.  This is a useful way to reason about systems.&lt;/p&gt;
&lt;p&gt;As I said, learning that the sums of exponentials was distributed as Gamma was a neat and useful
eureka! moment. However, until recently, I missed an extremely important related fact.  All of this
came about when I found a need to perform a computationally efficient test to determine if the
distributions two sets of exponentially distributed data differed. This lead to another eureka!
moment that I will describe below.&lt;/p&gt;
&lt;h3&gt;The Gamma distribution&lt;/h3&gt;
&lt;p&gt;Most folks are familiar with the Gamma function, which extends the factorial function to real numbers
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\Gamma(z) &amp;amp;= \int_0^{\infty} t^{z-1} e^{-t} dt\nonumber\\
&amp;amp;= (z-1)!,\, z \in \mathbb{Z}.\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;But fewer people seem to know about the Gamma distribution, which is defined to be
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(k, \theta) \equiv \frac{1}{\Gamma(k)\,\theta^k} x^{k-1} e^{-x/\theta}\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(k\)&lt;/span&gt; is the scale and &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is the shape. The Gamma distribution can be seen as a
generalization of two other daily-driver type distributions 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(k/2, 2) &amp;amp;= \frac{1}{\Gamma(k/2)\,2^{k/2}} x^{k/2-1} e^{-x/2}\nonumber\\
&amp;amp;\equiv \chi_k^2\nonumber\\
G(1, \theta) &amp;amp;= \frac{1}{\theta} e^{-x/\theta}\nonumber\\
&amp;amp;\equiv Exp(\lambda = 1/\theta)\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;The generality of the Gamma goes even further. Not only is it is it's own conjugate prior, it's the
conjugate prior for a large number of important distributions: Poisson, Normal, Pareto, Log-normal,
Exponential.&lt;/p&gt;
&lt;p&gt;But let's get back to proving some neat things about this distribution. Before we can start, we will
need one additional piece of machinery.  In
&lt;a href="/coupon-collector.html"&gt;The Coupon Collector&lt;/a&gt;), I showed a useful application for
probability generating functions (PGFs), which, to review, are&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \sum_0^\infty p(k)z^k\nonumber\\
&amp;amp;= \mathbb{E} \left[ z^k \right]\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;under the discrete distribution &lt;span class="math"&gt;\(p_k\)&lt;/span&gt;.  A handy generalization of PGFs is create moment generating
functions (MGFs), by taking&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_X (t) &amp;amp;\equiv G_X(e^t)\nonumber\\
&amp;amp;= \mathbb{E}\left[ e^{tX}\right].\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;MGFs have all the nice manipulation tricks of PGFs, have even nicer derivative-taking properties
than PGFs, and work for both continuous and discrete distributions.  And as long as we are careful
about integrating over the support of &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, they also have the very interesting expression as&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_X (t) &amp;amp;= \int_{-\infty}^{\infty} e^{tx} f(x) dx\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;which should bring back waves of nostalgia as the two sided Laplace transform of the distribution
&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;.  &lt;a href="/coupon-collector.html"&gt;Previously&lt;/a&gt;, we saw that one answer to "why did
I do all of these contortions on PGFs" was that taking derivatives gave us nice and easy ways to
estimate the mean, variance, etc. The MGFs also give us these as the moments of the distribution.
But the definition above provides us another trick, namely that if we are careful about the the
above definition, we can take the inverse Laplace transform of a MGF to derive a
distribution. Deriving distributions can be trickery, so this is a nice tool to have in the box. We
may return to calculating &lt;span class="math"&gt;\(\mathcal{L}^{-1}\left\{ M_X(t) \right\}\)&lt;/span&gt; in another post.&lt;/p&gt;
&lt;p&gt;So, what is the MGF for the Gamma distribution? By definition we have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_{G(k,\theta)}(t) &amp;amp;= \mathbb{E} \left[ e^{tx} \right]\nonumber\\
&amp;amp;= \int_0^\infty e^{tx} G\left(x \vert k,\theta\right) dx\nonumber\\
&amp;amp;= \frac{1}{\Gamma(k)\, \theta^k} \int_0^\infty x^{k-1} e^{-x\left(1/\theta - t\right)} dx, \, t \theta &amp;lt; 1\nonumber\\
&amp;amp;= \frac{1}{\Gamma(k)\, \theta^k\, (1/\theta - t)^k} \int_0^\infty x^{k-1} e^{-x} dx\nonumber\\
&amp;amp;= \frac{1}{\Gamma(k)\, \theta^k\, (1/\theta - t)^k} \Gamma(k)\nonumber\\
&amp;amp;= (1 - \theta t)^{-k}\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;With that we are ready for the first result.&lt;/p&gt;
&lt;h4&gt;Remark 1&lt;/h4&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(X_i \sim G(a_i, \theta)\)&lt;/span&gt;, then
&lt;/p&gt;
&lt;div class="math"&gt;$$ S_n = \sum_n X_i \sim G\left(\sum_{i=1}^n a_i, \theta\right)\nonumber\\ $$&lt;/div&gt;
&lt;h5&gt;Proof&lt;/h5&gt;
&lt;p&gt;By the independence of the &lt;span class="math"&gt;\(X_i\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_{S_n}(t) &amp;amp;= \mathbb{E}\left[ e^{t S_n} \right]\nonumber\\
&amp;amp;= \mathbb{E}\left[ \prod_{i=1}^n e^{t x_i} \right]\nonumber\\
&amp;amp;= \prod_{i=1}^n \mathbb{E}\left[e^{t x_i} \right]\nonumber\\
&amp;amp;= \prod_{i=1}^n (1- \theta t)^{-a_i} \nonumber\\
&amp;amp;= (1- \theta t)^{-\sum a_i} \nonumber\\
&amp;amp;= M_{G(\sum a_i, \theta)}\nonumber.\\
\end{align}&lt;/div&gt;
&lt;p&gt;We identify the result as the MGF for &lt;span class="math"&gt;\(G(\sum a_i, \theta)\)&lt;/span&gt;. &lt;span class="math"&gt;\(\blacksquare\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This result implies my original eureka! fact, namely that if &lt;span class="math"&gt;\(X_i \sim Exp(\lambda = 1/\theta) =
G(1, \theta)\)&lt;/span&gt;, then &lt;/p&gt;
&lt;div class="math"&gt;$$ S_n = \sum_n X_i \sim G\left(n, \theta\right).  $$&lt;/div&gt;
&lt;p&gt;Incidentally this also means that if &lt;span class="math"&gt;\(X_i \sim \chi^2_{n_i} = G(n_i/2, 2)\)&lt;/span&gt;, then the sum over those variables&lt;/p&gt;
&lt;div class="math"&gt;$$ S_n = \sum_n X_i \sim G \left( 1/2 \sum n_i, 2\right) = \chi^2_{\sum n_i} $$&lt;/div&gt;
&lt;p&gt;We need one more interesting property of the Gamma distribution to get to the finish line.&lt;/p&gt;
&lt;h4&gt;Remark 2&lt;/h4&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(X \sim G(k, \theta)\)&lt;/span&gt;, then
&lt;/p&gt;
&lt;div class="math"&gt;$$ cX \sim G(k, c \theta). $$&lt;/div&gt;
&lt;p&gt;Using Remark 2, we see that for &lt;span class="math"&gt;\(X_i \sim Exp(\lambda = 1/ \theta)\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;$$
S_n = \sum_{i=1}^{n} X_i \sim G(n, \theta) 
$$&lt;/div&gt;
&lt;p&gt;implies that 
&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{2}{\theta} S_n \sim G(n, 2) = \chi^2_{2n}. $$&lt;/div&gt;
&lt;p&gt;When I first saw this, I thought it was just amazing.  Now let me explain why.&lt;/p&gt;
&lt;h3&gt;Making sense of program run-times&lt;/h3&gt;
&lt;p&gt;As mentioned above, one of my current projects started all of this thinking.  I have been working on
a simple library to do performance-based regression testing, which I will talk more about that in a
later post, but for now, I present below some of the benchmarking data that I have been working
with. Each histogram is labeled by a number which is &lt;span class="math"&gt;\(k = log N\)&lt;/span&gt; of the total input size of the code
being tested. Specifically, I am benchmarking a library that uses Intel SSE4.2 specialized
instructions to calculate crc32c checksums.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A Gamma" src="/images/hists.png"&gt;&lt;/p&gt;
&lt;p&gt;There are a ton of interesting details going on that I will discuss later, but it should be clear
that for each &lt;span class="math"&gt;\(k\)&lt;/span&gt;, every time I make these measurements I will get different looking
histograms. They could be different because something changed in the code. Or, they could be
different just because we are dealing with random variables. All I really want to know is if
something has changed in the code. This will allow me to fail the regression test with some
meaningful statistical confidence. For profiling reasons, I might be interested in the parameters,
but primarily I need to detect changes.&lt;/p&gt;
&lt;p&gt;The usual way to deal with this kind of data situation is to take large number of samples &lt;span class="math"&gt;\(n_k\)&lt;/span&gt;. But
because my testing library needs to run very fast, taking a large number of sample is unacceptable
since the total test run time is proportional to the number of samples collected.  I need to test my
histogram sets against each other using as few total &lt;span class="math"&gt;\(\sum_k n_k\)&lt;/span&gt; as possible. Mainly this is
because in my experience, the probability of a test being manually disabled is proportional to the
time it takes to run the test. We need a system that no one every think about disabling.  Also, my
personal development style is to run the all dependent test-suites at every compile, not just at
every check-in. So, it needs to be fast.&lt;/p&gt;
&lt;p&gt;This is a good time to point out that by now most people looking at data like these have gone ahead
and assumed the values are Gaussian and have computed a &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (and probably a &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;).  It is a
terrible but fortuitous fact of life that the MLE for &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; for an exponential distribution is&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{\lambda} = \frac{n}{\sum_i x_i} = \frac{1}{\bar{x}}
$$&lt;/div&gt;
&lt;p&gt;I'm declaring this terrible because it basically lets most people get away with taking averages of
data like the above just assuming that they are quote-un-quote Gaussian. But I digress.&lt;/p&gt;
&lt;p&gt;What I need is a meaningful statistical test that can help me detect a change in the code.  This
brings me to why I got excited when I learned that&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{2}{\theta} S_n \sim G(n, 2) = \chi^2_{2n}. $$&lt;/div&gt;
&lt;p&gt;Seeing a &lt;span class="math"&gt;\(\chi^2\)&lt;/span&gt; gives me hope. It turns out that it works very well. If &lt;span class="math"&gt;\(X_1 \sim \chi^2_{n_1}\)&lt;/span&gt;
and &lt;span class="math"&gt;\(X_2 \sim \chi^2_{n_2}\)&lt;/span&gt;, then&lt;/p&gt;
&lt;div class="math"&gt;$$ U = \frac{X_1 / n_1}{X_2 / n_2} \sim F(2 n_1, 2 n_2) $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(F\)&lt;/span&gt; is the &lt;span class="math"&gt;\(F\)&lt;/span&gt;-distribution.  In our case, &lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
U &amp;amp;= \frac{\frac{2 S_{n'}}{\theta' n'}}{\frac{2 S_n}{\theta n}}\nonumber\\
&amp;amp;= \frac{\theta}{\theta'} \frac{\bar{x}'}{\bar{x}}\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;is distributed in &lt;span class="math"&gt;\(F(2n',2n)\)&lt;/span&gt;.  Thus we test &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; vs &lt;span class="math"&gt;\(H_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
H_0 &amp;amp;: \theta/\theta' = 1\nonumber\\
H_1 &amp;amp;: \theta/\theta' \neq 1.\nonumber\\
\end{align}&lt;/div&gt;
&lt;p&gt;Using tables or evaluation of &lt;span class="math"&gt;\(F\)&lt;/span&gt;, we can then derive a two sided &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value and also confidence
bounds. The &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value will be found from&lt;/p&gt;
&lt;div class="math"&gt;$$
p = 2 P(F(2n',2n) &amp;lt; U)
$$&lt;/div&gt;
&lt;p&gt;whereas the confidence &lt;span class="math"&gt;\(95%\)&lt;/span&gt; interval is&lt;/p&gt;
&lt;div class="math"&gt;$$
P\left(\frac{U}{T_{0.975}} &amp;lt; \theta/\theta' &amp;lt; \frac{U}{T_{0.025}} \right) = 0.95
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(T_{0.975}\)&lt;/span&gt; and &lt;span class="math"&gt;\(T_{0.025}\)&lt;/span&gt; are the &lt;span class="math"&gt;\(m\)&lt;/span&gt; quantiles on &lt;span class="math"&gt;\(F(2n', 2n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;That's it for now. I'll explain how I'm using this in another post.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "2em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Probability"></category><category term="Statistics"></category><category term="Generating Functions"></category></entry></feed>