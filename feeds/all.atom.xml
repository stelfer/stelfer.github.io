<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>memoize.me</title><link href="http://memoizeme.com/blog/" rel="alternate"></link><link href="http://memoizeme.com/blog/feeds/all.atom.xml" rel="self"></link><id>http://memoizeme.com/blog/</id><updated>2018-11-02T00:00:00-07:00</updated><entry><title>N-queens, Hamiltonian paths in Cayley Graphs, and Bell Ringing</title><link href="http://memoizeme.com/blog/nq-cayley.html" rel="alternate"></link><published>2018-11-02T00:00:00-07:00</published><updated>2018-11-02T00:00:00-07:00</updated><author><name>Soren Telfer</name></author><id>tag:memoizeme.com,2018-11-02:/blog/nq-cayley.html</id><summary type="html">&lt;p&gt;Every so often, I return to my solvers for combinatorial puzzles like Sudoku. There's another game
that has a storied history in computer science, going back to Djikstra's
&lt;a href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD03xx/EWD316.9.html"&gt;introduction of backtracking&lt;/a&gt; . The problem is &lt;a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle"&gt;Eight Queens&lt;/a&gt;. The idea is to
find every position of a chess board that can support the simultaneous placement of eight queen
pieces. It turns out that there are 12 distinct positions that solve this problem. But if you look
at all of the possible solutions you find by backtracking, you actually find 96 solutions. The
difference comes from the internal symmetries of the chess board …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Every so often, I return to my solvers for combinatorial puzzles like Sudoku. There's another game
that has a storied history in computer science, going back to Djikstra's
&lt;a href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD03xx/EWD316.9.html"&gt;introduction of backtracking&lt;/a&gt; . The problem is &lt;a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle"&gt;Eight Queens&lt;/a&gt;. The idea is to
find every position of a chess board that can support the simultaneous placement of eight queen
pieces. It turns out that there are 12 distinct positions that solve this problem. But if you look
at all of the possible solutions you find by backtracking, you actually find 96 solutions. The
difference comes from the internal symmetries of the chess board.&lt;/p&gt;
&lt;p&gt;You can actually try this game for any number of queens, on any size board. This generalization is
call the n-queens problem. The non-distinct solutions for 6 queens on a 6x6 board is shown below.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="images/6-queens.png"&gt;

&lt;figcaption&gt;

All solutions to placing 6 queens on a 6x6 chess board. The numbers indicate the number of queens
attacking that square.

&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;By starting at the top left, you can see that by rotating the board clockwise, you arrive at the top
right figure. By flipping horizontally, you arrive at the bottom left figure. By rotating
counter-clockwise, you arrive at the bottom right board.  There are different ways of dealing with
of pruning the non-unique boards, but the last time I played with my solver for n-queens, I tried
something I hadn't thought of before. This post will describe that idea.&lt;/p&gt;
&lt;p&gt;Generating permutations is something I've always found fascinating. I spent many hours in my youth
(and adulthood) writing page after page of permutations trying to find a new way to algorithmically
generate them.  When I started looking at checking the n-queens solutions for symmetries, I began to
think of connections between generating permutations and checking symmetries. This is what I will
describe in this post. A jupyter notebook using what's discussed here can be found on my
&lt;a href="https://github.com/stelfer/nqueens/blob/master/nqueens_D4.ipynb"&gt;github&lt;/a&gt;. Pruning the backtracking by evaluating symmetries gave a 32x speedup for &lt;span class="math"&gt;\(n=6\)&lt;/span&gt;
(at the end of the notebook).&lt;/p&gt;
&lt;h3&gt;Symmetries of a Square&lt;/h3&gt;
&lt;p&gt;So, what's the symmetry of a chess board anyway? The symmetries of regular polygons are captured by
the &lt;a href="https://en.wikipedia.org/wiki/Dihedral_group"&gt;Dihedral group&lt;/a&gt;, &lt;span class="math"&gt;\(D_n\)&lt;/span&gt;. In particular, the symmetries of a n-gon is captured by
&lt;span class="math"&gt;\(D_n\)&lt;/span&gt;. A chessboard is an regular 4-gon, so it's symmetries are captured by &lt;span class="math"&gt;\(D_4\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;What do I mean that &lt;span class="math"&gt;\(D_4\)&lt;/span&gt; captures the symmetries of a square? &lt;span class="math"&gt;\(D_4\)&lt;/span&gt; is generated by clock-wise and
counter-clockwise rotations of 90° and reflections about the vertical, horizontal and diagonal axes
of the square.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="images/d4.png"/&gt;
&lt;figcaption&gt;All of the symmetries of the square can be found by reflecting across the blue lines, or rotating clockwise or counter-clockwise around the center&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If you think about that figure for a bit, you'll see that there are many ways to end up in the same
place. For instance, rotating the square clockwise puts the dark grey square in the bottom right
corner, but so does a reflection about the horizontal axis. Groups are full of these kind of hidden
and complicated relationships.  A &lt;a href="https://en.wikipedia.org/wiki/Generating_set_of_a_group"&gt;generating set&lt;/a&gt; captures the idea of a specific set
of transformations that are sufficient to generate the group.&lt;/p&gt;
&lt;p&gt;If you've never seen this before, the systematic exploration of this is what group
theory is all about. I recommend &lt;a href="https://www.amazon.com/Visual-Group-Theory-Problem-Book/dp/088385757X"&gt;Carter&lt;/a&gt; for a friendly and geometrical introduction. For
an accessible introduction to generating sets, I recommend Keith Conrad's
&lt;a href="http://www.math.uconn.edu/~kconrad/blurbs/grouptheory/genset.pdf"&gt;notes&lt;/a&gt;. Keith has a wealth of good reading in his &lt;a href="http://www.math.uconn.edu/~kconrad/blurbs/"&gt;blurbs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is a visual way of encoding all of these relationships for a particular group, called a
&lt;a href="https://en.wikipedia.org/wiki/Cayley_graph"&gt;Cayley graph&lt;/a&gt;. Cayley graphs use a particular generating set of transformations (like
reflection and rotation above), and show how you can arrive at every symmetry of your starting point
by sequentially applying transformations. The Wikipedia article on Cayley graphs actually has a
graph for &lt;span class="math"&gt;\(D_4\)&lt;/span&gt; in the example section&lt;/p&gt;
&lt;figure&gt;

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Dih_4_Cayley_Graph%3B_generators_a%2C_b.svg/403px-Dih_4_Cayley_Graph%3B_generators_a%2C_b.svg.png"&gt;
&lt;figcaption&gt;

A Cayley graph of D4 using clockwise rotation (a) and horizontal reflection (b).

&lt;figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To see what's happening here, start at the F labeled &lt;span class="math"&gt;\(e\)&lt;/span&gt;.  By applying &lt;span class="math"&gt;\(a\)&lt;/span&gt;, we go up along the red
arrow and we get an F that has been rotated 90 degrees in the clockwise direction. If you then
follow the blue line up and to the left, you are applying a horizontal reflection &lt;span class="math"&gt;\(b\)&lt;/span&gt;. If you moved
back along the blue line, you would go back to where you were before, and algebraically you would be
at &lt;span class="math"&gt;\(abb\)&lt;/span&gt;. But since &lt;span class="math"&gt;\(bb = 1\)&lt;/span&gt;, i.e. two reflection across the same axis take you back to the same
place, you end up algebraically back at &lt;span class="math"&gt;\(a\)&lt;/span&gt;. Rotations don't work like reflections. Two clockwise 90
degree rotations don't take you back where you started. Instead, &lt;span class="math"&gt;\(aaaa = 1\)&lt;/span&gt;. This is why the red
arrows have directions, and why there's an outside square and inside square of red arrows, that give
&lt;span class="math"&gt;\(aaaa\)&lt;/span&gt; to get you around to where you started.&lt;/p&gt;
&lt;p&gt;So, what does this have to do with my original interest in symmetries of n-queens? Well, say we
start with a configuration of queens, such as in the top right solution above, how would I
algorithmically take that board on a tour of every symmetry it has under &lt;span class="math"&gt;\(D_4\)&lt;/span&gt;? If you look at the
Cayley graph above, you can imagine starting at &lt;span class="math"&gt;\(e\)&lt;/span&gt; and applying &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; in a way to take us to
every other node. But how do we do this in a systematic way? How can we only visit each node once?
The answer from that Cayley graph is not obvious.&lt;/p&gt;
&lt;p&gt;In computer science terms, we're asking of there exists at least one Hamiltonian path in that
graph, and if so, how do we find it. Finding Hamiltonian paths in graphs is one of the classic
NP-complete problems of computer science. We don't actually know if every Cayley graph is
Hamiltonian, but the &lt;a href="https://en.wikipedia.org/wiki/Lov%C3%A1sz_conjecture"&gt;Lovász conjecture&lt;/a&gt; posits that they are. And, the Lovász conjecture
has been proven for almost all cases of finite Cayley graphs. So, this gives us hope, but a
Hamiltonian path on Cayley graph for &lt;span class="math"&gt;\(D_4\)&lt;/span&gt; above it isn't immediately obvious, at least for me.&lt;/p&gt;
&lt;p&gt;Remember that we only used two of the many possible transformations to make that graph. What about
if we used others, what would happen? Well, Wikipedia is way ahead of us. They display another
Cayley graph for &lt;span class="math"&gt;\(D_4\)&lt;/span&gt; on the same page:&lt;/p&gt;
&lt;figure&gt;
&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Dih_4_Cayley_Graph%3B_generators_b%2C_c.svg/300px-Dih_4_Cayley_Graph%3B_generators_b%2C_c.svg.png"/&gt;

&lt;figcaption&gt;

A Cayley graph of D4 using a different generating set: b as before, but c = ba instead of a.

&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Here &lt;span class="math"&gt;\(c = ba\)&lt;/span&gt;. You may also recognize it as a reflection along the diagonal axis running from SE to
NW (or between the two light grey boxes in the figure above). Notice also that in writing &lt;span class="math"&gt;\(c = ba\)&lt;/span&gt;,
the order of &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; matter. Except for &lt;span class="math"&gt;\(D_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(D_2\)&lt;/span&gt;, the generators of &lt;span class="math"&gt;\(D_n\)&lt;/span&gt; do not
commmute, i.e. &lt;span class="math"&gt;\(D_n\)&lt;/span&gt; for &lt;span class="math"&gt;\(n &amp;gt; 2\)&lt;/span&gt; are &lt;a href="https://en.wikipedia.org/wiki/Abelian_group"&gt;non abelian&lt;/a&gt;. You can see this from the
figure above. If you look at &lt;span class="math"&gt;\(bc\)&lt;/span&gt; and &lt;span class="math"&gt;\(cb\)&lt;/span&gt;, you'll see they aren 't the same thing. The non abelian
structure generally makes the Cayley graph a hall of mirrors when you are trying to find a path through it.&lt;/p&gt;
&lt;p&gt;At any rate, in this Cayley graph, you can immediately see the circuit we can take to evaluate all
of the symmetries of a n-queens position. Typically in an n-queens solver you represent the board as
an &lt;span class="math"&gt;\(n \times n\)&lt;/span&gt; matrix. in this setting, evaluating reflections is as simple as swapping matrix
entries seven times. In my solver, every time I find a solution by backtracking, I quickly check the
board against solutions that I've saved before. Doing this at the time of finding a solution greatly
speeds up the backtracking since it substantially prunes the tree.&lt;/p&gt;
&lt;h3&gt;Hamiltonian Paths for Permutations&lt;/h3&gt;
&lt;p&gt;As I mentioned before, I've always been interested in generating permutations. Just as the
symmetries of a square are captured by &lt;span class="math"&gt;\(D_4\)&lt;/span&gt;, The &lt;span class="math"&gt;\(n!\)&lt;/span&gt; ways of arranging &lt;span class="math"&gt;\(n\)&lt;/span&gt; objects is captured by
the &lt;a href="https://en.wikipedia.org/wiki/Symmetric_group"&gt;symmetric group&lt;/a&gt; &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; on &lt;span class="math"&gt;\(n\)&lt;/span&gt; objects. The symmetric group is of fundamental
importance in group theory due to it's broad application, and by
&lt;a href="https://en.wikipedia.org/wiki/Cayley%27s_theorem"&gt;Cayley's Theorem&lt;/a&gt;, which provides a correspondence between &lt;em&gt;any&lt;/em&gt; group and
subgroups of the symmetric group.&lt;/p&gt;
&lt;p&gt;Wikipedia gives us a Cayley graph for &lt;span class="math"&gt;\(S_4\)&lt;/span&gt;, the symmetric group on four items.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Symmetric_group_4%3B_Cayley_graph_4%2C9.svg/812px-Symmetric_group_4%3B_Cayley_graph_4%2C9.svg.png" width="25%"/&gt;

&lt;figcaption&gt;

A Cayley graph for S4. Red arrow rotates the items to the right, while the blue arrow swaps the first and second, then second and third items.

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This time &lt;span class="math"&gt;\(e\)&lt;/span&gt; is in the bottom left corner at &lt;span class="math"&gt;\(1234\)&lt;/span&gt;. A red arrow takes every item and moves it one
position to the right, with the ends wrapping around to the other side.  This type of transformation
is called a path. A blue arrow performs two trandsformations, first swapping the the first and
second item, then the second and third item. The fourth stays the same. These types of
transformations are called transpositions, or I call them swaps.&lt;/p&gt;
&lt;p&gt;As another way of visualizing permutations, we can look at the notation used for cycles and
swaps. Over &lt;span class="math"&gt;\(S_3\)&lt;/span&gt;, a 3-cycle that simultaneously moves the first to the second, second to third,
etc, we write &lt;span class="math"&gt;\((123)\)&lt;/span&gt;. If we write out the &lt;span class="math"&gt;\(3! = 6\)&lt;/span&gt; 3-cycles of &lt;span class="math"&gt;\(S_3\)&lt;/span&gt;, we get &lt;/p&gt;
&lt;div class="math"&gt;$$ 
(123)\\
(132)\\
(213)\\
(231)\\
(312)\\
(321)\\
$$&lt;/div&gt;
&lt;p&gt;Some of these do the same thing, for instance &lt;span class="math"&gt;\((123)\)&lt;/span&gt; and &lt;span class="math"&gt;\((231)\)&lt;/span&gt;, but &lt;span class="math"&gt;\((123)\)&lt;/span&gt; and &lt;span class="math"&gt;\((132)\)&lt;/span&gt; do
something different. But what's clear from writing it out that we have a cycle for each actual
permutation of 3 objects.  Because of this, we conclude that the cyles of any permutation group
generates the group. Taking the cycles as the generating set has an obvoius disadvantage when you
are trying to algorithmically generate permutations: you have to already have the permutations to
make the generating set. It would be nice to be able to use a more compact set of
generators. Luckily this is possible.&lt;/p&gt;
&lt;p&gt;Generating permutations exhaustively requires &lt;span class="math"&gt;\(n!\)&lt;/span&gt; amount of work. For large &lt;span class="math"&gt;\(n\)&lt;/span&gt;, this can get
literally astronomical.  I mentioned the &lt;a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle"&gt;Fischer-Yates shuffle&lt;/a&gt; in the post on the
&lt;a href="coupon-collector.html"&gt;Coupon Collector&lt;/a&gt;.  If you are not familiar with that algorithm, it uniformly generates random
permutations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;swap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Part of the reason I have spent so much time thinking about F-Y because of &lt;a href="https://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena/dp/1849967202"&gt;&lt;em&gt;Skiena&lt;/em&gt;&lt;/a&gt;, where he asks why the following does not generate uniformly random permutations:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;swap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Analyzing the algorithm, in particular why it generates uniformly random permutations, and why this
second snippet doesn't, is my favorite interview question. Maybe I will post about that later. But
for now, taking a step back, why do we think that generating permutations by swaps would even work?&lt;/p&gt;
&lt;p&gt;Well we've seen that swaps have a role to play in the Cayley graph above for &lt;span class="math"&gt;\(S_4\)&lt;/span&gt;. But that
generating set also uses cycles. F-Y doesn't use cycles, just swaps. Using a similar formalism to
the way we write cycles, we can also write swaps. In particular, we write the exchange of the first
and second elements as &lt;span class="math"&gt;\((12)\)&lt;/span&gt;. A moments reflection tells us that:&lt;/p&gt;
&lt;div class="math"&gt;$$
(1234) = (12)(23)(34) = \sigma_{12} \sigma_{23} \sigma_{34}\\
$$&lt;/div&gt;
&lt;p&gt;I wrote the &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;'s because we are going to watch them in action and don't want to create too
much confusion with paranthesis. Note also that the permutations apply by acting on the right.  We
will also use letters rather than numbers to keep the confusion at a minimum&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\sigma_{34}(ABCD) &amp;amp;\rightarrow ABDC\\
\sigma_{23}(ABDC) &amp;amp;\rightarrow ADBC\\
\sigma_{12}(ADBC) &amp;amp;\rightarrow DABC.\\
\end{aligned}&lt;/div&gt;
&lt;p&gt;Which gives us the same result as the cycle &lt;span class="math"&gt;\((1234)\)&lt;/span&gt;. So, cycles are products of &lt;span class="math"&gt;\(n-1\)&lt;/span&gt; swaps.  Since
cycles generate symmetry groups, that means that swaps also generate the symmetry groups. We can now
see why F-Y is a good idea, or at least why it might work.&lt;/p&gt;
&lt;p&gt;There are &lt;span class="math"&gt;\(\binom{n}{2} = \frac{n(n-1)}{2}\)&lt;/span&gt; swaps for &lt;span class="math"&gt;\(S_n\)&lt;/span&gt;. So, if we want to use swaps to
algorithmically generate permutations, then we only have to do &lt;span class="math"&gt;\(\binom{n}{2}\)&lt;/span&gt; work finding the
generators before we can start making permutations. This saves a lot of work. But we can do even
better. It turns out that for any &lt;span class="math"&gt;\(n \geq 2\)&lt;/span&gt;, &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; is generated by &lt;span class="math"&gt;\((12)\)&lt;/span&gt; and &lt;span class="math"&gt;\((12...n)\)&lt;/span&gt;. A proof
is found at Keith Conrad's &lt;a href="http://www.math.uconn.edu/~kconrad/blurbs/grouptheory/genset.pdf"&gt;notes&lt;/a&gt;. This means we can use the same generating set for
any &lt;span class="math"&gt;\(n\)&lt;/span&gt;, and the only upfront work we have to do is in creating the cycle.&lt;/p&gt;
&lt;p&gt;Back to using swaps. One day a few years ago I was writing out permutations, and I found the
following pattern.  For any &lt;span class="math"&gt;\(n\)&lt;/span&gt;, write out the &lt;span class="math"&gt;\(n-1\)&lt;/span&gt; swaps in columns. Start with &lt;span class="math"&gt;\(e\)&lt;/span&gt;, for the case
of &lt;span class="math"&gt;\(S_4\)&lt;/span&gt;, that's &lt;span class="math"&gt;\(1234\)&lt;/span&gt;. Move the permutation to the right using the swap in the column until you
get to the end of a row. When you get to the right side of the table, go down with the first swap
&lt;span class="math"&gt;\((12)\)&lt;/span&gt;. Then go back across the table the other direction until you hit the left side of the
table. Then go down with the last swap of the set. Keep this up until you get back to &lt;span class="math"&gt;\(e\)&lt;/span&gt;. This
process is shown below.&lt;/p&gt;
&lt;div class="math"&gt;\begin{array}{c|c|c|c|c|c|c|c|c}
\def\ra{{\rightarrow}}
\def\la{{\leftarrow}}
\def\da{{\downarrow}}
     &amp;amp;      &amp;amp; (12) &amp;amp;      &amp;amp; (23) &amp;amp;      &amp;amp; (34) &amp;amp;      &amp;amp;      \\ \hline
     &amp;amp; 1234 &amp;amp; \ra  &amp;amp; 2134 &amp;amp; \ra  &amp;amp; 2314 &amp;amp; \ra  &amp;amp; 2341 &amp;amp;      \\
     &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp; \da  &amp;amp; (12) \\
     &amp;amp; 1324 &amp;amp; \la  &amp;amp; 3124 &amp;amp; \la  &amp;amp; 3214 &amp;amp; \la  &amp;amp; 3241 &amp;amp;      \\
(34) &amp;amp; \da  &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      \\
     &amp;amp; 1342 &amp;amp; \ra  &amp;amp; 3142 &amp;amp; \ra  &amp;amp; 3412 &amp;amp; \ra  &amp;amp; 3421 &amp;amp;      \\
     &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp; \da  &amp;amp; (12) \\
     &amp;amp; 1432 &amp;amp; \la  &amp;amp; 4132 &amp;amp; \la  &amp;amp; 4312 &amp;amp; \la  &amp;amp; 4321 &amp;amp;      \\
(34) &amp;amp; \da  &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      \\  
     &amp;amp; 1423 &amp;amp; \ra  &amp;amp; 4123 &amp;amp; \ra  &amp;amp; 4213 &amp;amp; \ra  &amp;amp; 4231 &amp;amp;      \\
     &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp; \da  &amp;amp; (12) \\
     &amp;amp; 1243 &amp;amp; \la  &amp;amp; 2143 &amp;amp; \la  &amp;amp; 2413 &amp;amp; \la  &amp;amp; 2431 &amp;amp;      \\
(34) &amp;amp; \da  &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      &amp;amp;      \\      
     &amp;amp; 1234 &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp; &amp;amp;      \\
\end{array}&lt;/div&gt;
&lt;p&gt;This works for any &lt;span class="math"&gt;\(n\)&lt;/span&gt;. So, this is an algorithmic way to construct a Hamiltonian path on the
Cayley graph for &lt;span class="math"&gt;\(S_n\)&lt;/span&gt;. It allows us to systematically generate permutations. At first I was
excited, but after some research I learned that this was figured out in the 1960's. This algorithm is
called the Johnson-Steinhaus-Trotte (JST) solution to the motel problem. The motel problem comes from
Martin Gardner's &lt;a href="https://www.amazon.com/Time-Travel-Other-Mathematical-Bewilderments/dp/0716719258"&gt;Time Travel and other Mathematical Bewilderments&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Mr. Smith manages a motel. It consists of n rooms in a straight row. There is no vacancy. Smith is a psychologist who plans to study the effects of rearranging his guests in all possible ways. Every morning he gives them a permutation. The weather is miserable, raining almost daily. To minimize his guests’ discomfort, each daily rearrangement is made by exchanging only the occupants of two adjoining rooms. Is there a simple algorithm that will run through all possible rearrangements by switching only one pair of adjacent occupants at each step?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It turns out this problem is even older than that. &lt;a href="https://arxiv.org/abs/1203.1835"&gt;&lt;em&gt;McGuire&lt;/em&gt;&lt;/a&gt; describes the history of this problem dating back to the 17th century:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose there are n bells being rung, numbered 1,2,3,...,n in order of pitch, number 1 being the highest. When the bells are rung in order of descending pitch 1, 2, 3, . . . , n, we say they are being rung in rounds. A change in the order of the bells, such as rounds 1 2 3 4 5 being changed to 2 1 4 3 5, can be considered as a permutation in the symmetric group on five objects. In the years 1600–1650 a new craze emerged where the ringers would continuously change the order of the bells for as long as possible, while not repeating any particular order, and return to rounds at the end. This game evolved into a challenge to ring the bells in every possible order, without any repeats, and return to rounds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;McGuire&lt;/em&gt; plausibly claims that the method above was actually discovered by bell-ringers in the
1650's. JST only solves one variant of the large topic of &lt;a href="https://en.wikipedia.org/wiki/Change_ringing"&gt;change ringing&lt;/a&gt;. &lt;em&gt;McGuire&lt;/em&gt;
provides a nice history of this and ties it back to Hamilton paths on Cayley graphs. The Simons
Foundation has a nice &lt;a href="https://www.youtube.com/watch?v=3lyDCUKsWZs"&gt;video&lt;/a&gt; on the topic.&lt;/p&gt;
&lt;figure&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/3lyDCUKsWZs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=default";
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Groups"></category><category term="Combinatorics"></category></entry><entry><title>What's coming next...</title><link href="http://memoizeme.com/blog/2018-11-01-next.html" rel="alternate"></link><published>2018-11-01T00:00:00-07:00</published><updated>2018-11-01T00:00:00-07:00</updated><author><name>Soren Telfer</name></author><id>tag:memoizeme.com,2018-11-01:/blog/2018-11-01-next.html</id><summary type="html">&lt;p&gt;I'm going to start doing this regularly in 2019. I'm making this post now to give myself a bit of
&lt;span class="math"&gt;\(v_{0}\)&lt;/span&gt; going into the new year. A short list of what's coming:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A while back I was messing with &lt;a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle"&gt;n-queens&lt;/a&gt;, and I found myself needing to test board
permutations against the internal symmetries of chess. That lead me back to one of my favorite
things to think about, generating permutations. This lead me to looking at
&lt;a href="https://en.wikipedia.org/wiki/Cayley_graph"&gt;Cayley graphs&lt;/a&gt; of &lt;span class="math"&gt;\(D_4\)&lt;/span&gt;, which lead me to &lt;a href="https://arxiv.org/abs/1203.1835"&gt;bell ringing&lt;/a&gt; from the 17th
century . I'll explain.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recently I found myself reading about &lt;a href="https://en.wikipedia.org/wiki/Belief_propagation"&gt;belief …&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=default";
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;I'm going to start doing this regularly in 2019. I'm making this post now to give myself a bit of
&lt;span class="math"&gt;\(v_{0}\)&lt;/span&gt; going into the new year. A short list of what's coming:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;A while back I was messing with &lt;a href="https://en.wikipedia.org/wiki/Eight_queens_puzzle"&gt;n-queens&lt;/a&gt;, and I found myself needing to test board
permutations against the internal symmetries of chess. That lead me back to one of my favorite
things to think about, generating permutations. This lead me to looking at
&lt;a href="https://en.wikipedia.org/wiki/Cayley_graph"&gt;Cayley graphs&lt;/a&gt; of &lt;span class="math"&gt;\(D_4\)&lt;/span&gt;, which lead me to &lt;a href="https://arxiv.org/abs/1203.1835"&gt;bell ringing&lt;/a&gt; from the 17th
century . I'll explain.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recently I found myself reading about &lt;a href="https://en.wikipedia.org/wiki/Belief_propagation"&gt;belief propagation&lt;/a&gt;. This lead me to a
&lt;a href="https://arxiv.org/abs/1702.00467"&gt;2017 paper&lt;/a&gt; from &lt;a href="http://tuvalu.santafe.edu/~moore/"&gt;Chris Moore&lt;/a&gt;, talking about the hardness of certain graph
problems in the setting of statistical physics and random graph theory. It lead me to thinking about
why Facebook needs trolls. I'm sure this is well trodden ground, but I'll explain my thoughts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I want to write up how I've learned to think about weather, particularly how I think about the
expected utility of purchasing a ski pass for Tahoe.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I recently read on HackerNews the assertion that "Nakamoto Consensus" was a "CS breakthrough" in
the &lt;a href="https://en.wikipedia.org/wiki/Byzantine_fault_tolerance#Byzantine_Generals'_Problem"&gt;Byzantine Generals Problem&lt;/a&gt;. That seems to me a bit strong, but I want to
explore in 2018 how much of that statement is true. I'm also interested how many young engineers out
there share this view.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the things we think about (and obliquely work on), is the role of Machine Learning in
entertainment and content creation. There's a narrative out there that Netflix told the creators of
House of Cards to make a show with Kevin Spacey in it because they knew it would be a hit.  That's
not &lt;a href="https://www.wired.com/2012/11/netflix-data-gamble/"&gt;actually what happend&lt;/a&gt;.  I'm interested in the history of this myth, vis-a-vis
the rise of Big Data, and the AI, as SV hype tribes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ultimately, I need to write about innovation. I've been running an innovation center for a $150B+
company for going on six years now. I have a lot of thoughts to share about innovation programs,
innovation in general, and the future of R&amp;amp;D.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=default";
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Planning"></category><category term="Motivation"></category></entry><entry><title>A Different View on the Coupon Collector Problem</title><link href="http://memoizeme.com/blog/coupon-collector.html" rel="alternate"></link><published>2018-10-27T00:00:00-07:00</published><updated>2018-10-27T00:00:00-07:00</updated><author><name>Soren Telfer</name></author><id>tag:memoizeme.com,2018-10-27:/blog/coupon-collector.html</id><summary type="html">&lt;p&gt;Once during a meeting in which we were discussing the &lt;a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle"&gt;Fischer-Yates shuffle&lt;/a&gt; someone
asked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How many times do we need to run the algorithm to see every permutation?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a really great question and it caught me off guard. The reason was that since Fischer-Yates
is a random algorithm, the answer really involves thinking about random processes. So, like, I can't
really answer that question as it is posed.  While thinking aloud as I struggled to answer, I
reduced the problem to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What's the average number of times I need to flip a coin in order to see both …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;Once during a meeting in which we were discussing the &lt;a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle"&gt;Fischer-Yates shuffle&lt;/a&gt; someone
asked:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How many times do we need to run the algorithm to see every permutation?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a really great question and it caught me off guard. The reason was that since Fischer-Yates
is a random algorithm, the answer really involves thinking about random processes. So, like, I can't
really answer that question as it is posed.  While thinking aloud as I struggled to answer, I
reduced the problem to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What's the average number of times I need to flip a coin in order to see both sides?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Enter The Coupon Collector Problem (CCP)&lt;/h3&gt;
&lt;p&gt;I know it says coupons, and I was just mentioning a coin, but let's talk about &lt;a href="https://en.wikipedia.org/wiki/Happy_Meal"&gt;Happy Meals&lt;/a&gt;.
Say you were a child in the early 80's and McDonald's started running one of their happy meal
promotions. They create a number (usually &lt;span class="math"&gt;\(\approx 10\)&lt;/span&gt;) of plastic trinkets that you get one of
every time you buy a happy meal. You, as the child, love them. Your parents, as the ones who have to
drive you to McDonald's, are wondering how many more times they need to drive you to McDonald's.  It
turns out than when you purchase a Happy Meal, instead of letting you pick which trinket your child
wants, they just randomly give you whichever one comes out of the bin of trinkets they have under
the counter.  In an earlier, more idyllic America, people apparently had to do this with coupons in
the newspaper.&lt;/p&gt;
&lt;p&gt;Regardless of how you want to think about it, let's say that there are &lt;span class="math"&gt;\(m\)&lt;/span&gt; trinkets, to which you
assign an arbitrary order and index &lt;span class="math"&gt;\(1 \leq i \leq m\)&lt;/span&gt;. The classic way of getting the result is
provided by &lt;a href="https://en.wikipedia.org/wiki/Coupon_collector%27s_problem"&gt;Wikipedia&lt;/a&gt;, which asks you to think about &lt;span class="math"&gt;\(t_i\)&lt;/span&gt;, the number of trials to get
the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th trinket after having completed the labor of obtaining the previous &lt;span class="math"&gt;\(i - 1\)&lt;/span&gt; trinkets.
What's the probability of getting the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th trinket? Well it's &lt;span class="math"&gt;\(p_i = (m - (i-1))/m\)&lt;/span&gt;. Then the
expected number of trials to get the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th is &lt;span class="math"&gt;\(1/p_i\)&lt;/span&gt;, just like the expected number of times to
roll a 1 on a six-sided die is 6. The total number of trials, then, is calculated by adding up all
of these individual exercises in obtaining trinkets. Let's call this total &lt;span class="math"&gt;\(T\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[T] &amp;amp;= \sum_{i=1}^m E[t_i] \\
&amp;amp;= \sum_{i=1}^m \frac{1}{p_i}\\
&amp;amp;= \frac{m}{m} + \frac{m}{m-1} + ... + \frac{m}{1}\\
&amp;amp;= m\left(1 + \frac{1}{2} + ... + \frac{1}{m}\right)\\
&amp;amp;= m H_m\\
\end{align}&lt;/div&gt;
&lt;p&gt;I'll be honest and say that I &lt;em&gt;really&lt;/em&gt; don't like this derivation. It seems like a trick. This
certainly isn't the way I think about sums of random variables. But this is way most people are
taught to solve this problem. In any event, I didn't answer the original question about
Fischer-Yates like this.  The rest of this post will take you on a seemingly circuitous path towards
something that is in the end very simple, and very beautiful.  I think it also proves that when all
else fails, brute force counting can take you a long way, provided you have the machinery to get you
there.&lt;/p&gt;
&lt;h3&gt;Flipping a coin to see both sides&lt;/h3&gt;
&lt;p&gt;So, let's get back to the coin. We can model the situation as a sequence of experiments in flipping
them.  These experiments generate sequences in which the &lt;span class="math"&gt;\(n\)&lt;/span&gt;-th event is &lt;span class="math"&gt;\(H\)&lt;/span&gt; or &lt;span class="math"&gt;\(T\)&lt;/span&gt; and the &lt;span class="math"&gt;\(n-1\)&lt;/span&gt;
other events are all either &lt;span class="math"&gt;\(T\)&lt;/span&gt; or &lt;span class="math"&gt;\(H\)&lt;/span&gt; respectively. What would these sequences look like?  If we
start enumerating the possibilities, we can see patterns appear, depending on how long we run the
experiments.  For the sequences of length at most &lt;span class="math"&gt;\(n\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_n = \,&amp;amp;HT &amp;amp;&amp;amp;+ TH\\
    + &amp;amp;HHT  &amp;amp;&amp;amp;+ TTH\\
    + &amp;amp;\ldots\\
    + &amp;amp;H^{n-1}T &amp;amp;&amp;amp;+ T^{n-1}H.\\
\end{align}&lt;/div&gt;
&lt;p&gt;It's interesting that the generation of length &lt;span class="math"&gt;\(k\)&lt;/span&gt; is easy to &lt;em&gt;generate&lt;/em&gt; from the &lt;span class="math"&gt;\((k-1)\)&lt;/span&gt;-th by
simply prepending the opposite of the terminal token, which doesn't change.  The production rule
here is quite simple.  Anyway, what do we do with this sum? Well, if we think about &lt;span class="math"&gt;\(H\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt; as
probabilities &lt;span class="math"&gt;\(P(H) = p\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(T) = q = 1 - p\)&lt;/span&gt;, then &lt;span class="math"&gt;\(S_\infty\)&lt;/span&gt; better sum to &lt;span class="math"&gt;\(1\)&lt;/span&gt;. So let's check
this by collecting terms:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_{\infty} &amp;amp;= T\left(H + H^2 + H^3 + \ldots\right) + H\left(T + T^2 + T^3 + \ldots\right)\\
&amp;amp;= T\left(\frac{1}{1-H} - 1\right) + H\left(\frac{1}{1-T} - 1\right)\\
&amp;amp;= HT\left(\frac{1}{1-H} + \frac{1}{1-T}\right)\\
&amp;amp;= pq \left(\frac{p+q}{pq}\right)\\
&amp;amp;= 1.\\
\end{align}&lt;/div&gt;
&lt;p&gt;On the second step, we've inserted the series representation for &lt;span class="math"&gt;\(\frac{1}{1-x}\)&lt;/span&gt; for &lt;span class="math"&gt;\(x \lt
1\)&lt;/span&gt;. Since we're thinking about &lt;span class="math"&gt;\(H\)&lt;/span&gt; and &lt;span class="math"&gt;\(T\)&lt;/span&gt; as probabilities here, we should feel OK about this. Pay
special attention to the third step, because we'll be using it again.  Now to calculate &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt; we
interpret &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; as a probability density, and evaluate the sum: &lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[n] &amp;amp;= \sum_{n=2}^\infty n S_n\\ 
&amp;amp;=\,2(pq + qp)\, + 3(ppq + qqp) + \ldots m(p^{m-1}q + q^{m-1}p) \ldots...\\
\end{align}&lt;/div&gt;
&lt;p&gt;We start the sum for &lt;span class="math"&gt;\(n=2\)&lt;/span&gt; since &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; does not have support for &lt;span class="math"&gt;\(n &amp;lt; 2\)&lt;/span&gt;. We can't see both sides of
a coin by doing a single flip. Regrouping a bit, we see that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[n] &amp;amp;= q\left(2p + 3p^2 + \ldots mp^{m-1} + \ldots\right) + p\left(2q + 3p^2 + \ldots mq^{m-1} + \ldots \right) \\
    &amp;amp;= \frac{q}{p} \left(\sum_{i=1}^\infty ip^i - p\right) + \frac{p}{q} \left(\sum_{i=1}^\infty iq^i - q\right)\\
&amp;amp;= \frac{q}{p}\left(\frac{p}{q^2} - p\right) + \frac{p}{q}\left(\frac{q}{p^2} - q\right)\\
&amp;amp;= \frac{p}{q}\left(\frac{1}{1-q} - q\right) + \frac{q}{p}\left(\frac{1}{1-p} - p\right)\\
&amp;amp;= \frac{1 - pq}{p(1-p)}\\
&amp;amp;= \frac{p^2 - p + 1}{p(1-p)}\\
&amp;amp;= 2 + \frac{p^3 + q^3}{pq}\\
\end{align}&lt;/div&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(p = q\)&lt;/span&gt; we have that &lt;span class="math"&gt;\(E[n] = 3\)&lt;/span&gt;. The average number of times you need to flip a fair coin to see
both sides is 3. If &lt;span class="math"&gt;\(p = 0.25\)&lt;/span&gt;, then &lt;span class="math"&gt;\(E[n] = 4.\bar{33}\)&lt;/span&gt;. If the coin is only 10% fair, then &lt;span class="math"&gt;\(E[n]
\approx 10\)&lt;/span&gt;. If you're interested in how it varies with &lt;span class="math"&gt;\(p\)&lt;/span&gt;:&lt;/p&gt;
&lt;figure&gt;
&lt;a name="fair-coin-expectation"&gt;&lt;/a&gt;
&lt;img src="images/fair-coin-expectation.png"/&gt;
&lt;figcaption&gt;
Expected number of flips of a coin to see both sides as a function of the bias of the coin.
&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This is maybe a way of estimating if a coin is &lt;em&gt;extremely&lt;/em&gt; unfair: build an estimator based on the
number flips it takes to see both sides.&lt;/p&gt;
&lt;h3&gt;Fine for coins, but what about the trinkets?&lt;/h3&gt;
&lt;p&gt;Having solved the problem for &lt;span class="math"&gt;\(m = 2\)&lt;/span&gt;, the next thing to do was to try for &lt;span class="math"&gt;\(m &amp;gt; 2\)&lt;/span&gt;.  First, though,
we need to think about the &lt;span class="math"&gt;\(p_k\)&lt;/span&gt;, since that's where it might get harry. If I were running a Happy
Meal campaign, I might think about how to manipulate the &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; in order to maximize &lt;span class="math"&gt;\(E[T]\)&lt;/span&gt;.  Some
options here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Naive approach: all equal probabilities, i.e. &lt;span class="math"&gt;\( p_i = 1/m\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Singularly hard to get: One distinctive &lt;span class="math"&gt;\(p_j\)&lt;/span&gt;, but all other &lt;span class="math"&gt;\(p\)&lt;/span&gt;'s equal&lt;/li&gt;
&lt;li&gt;Anything goes: arbitrary &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; subject to &lt;span class="math"&gt;\(\sum p_i = 1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think if you grew up in the early 80's you might have felt like McDonald's used the second
approach.  But it turns out that the first is a lot easier to analyze, and will carry us through to
the thing I want to show in this post.&lt;/p&gt;
&lt;p&gt;The first thing that happened when we tried to do brute-force counting for &lt;span class="math"&gt;\(m \gt 2\)&lt;/span&gt;, was that the
sequences quickly got out of hand. We needed a better approach. You may have noticed that I used the
word &lt;em&gt;generate&lt;/em&gt; above. That was a hint. &lt;a href="https://en.wikipedia.org/wiki/Generating_function"&gt;Generating Functions&lt;/a&gt; are an incredibly useful
tool in the toolbox of the brute-forcing recreational combinatorialist. They're also pretty
fear-inspiring to a lot of people. &lt;/p&gt;
&lt;p&gt;When I got tired of brute force counting the sequences for &lt;span class="math"&gt;\(m = 3\)&lt;/span&gt;, I remembered some advice from
&lt;a href="http://www.amazon.com/Concrete-Mathematics-Foundation-Computer-Science/dp/0201558025"&gt;&lt;em&gt;Knuth et. al.&lt;/em&gt;&lt;/a&gt; to use finite automata to help organize generating functions for
counting exercises like this. But before we get to that, it's first useful to introduce the
machinery that we will use when we get there.&lt;/p&gt;
&lt;h3&gt;The coin redux, this time as a PGF&lt;/h3&gt;
&lt;p&gt;I'm not going to do justice to generating functions in this space. I recommend &lt;em&gt;Knuth et. al.&lt;/em&gt; as a
gentle introduction and &lt;a href="https://www.amazon.com/generatingfunctionology-Third-Herbert-S-Wilf/dp/1568812795/"&gt;&lt;em&gt;Wilf&lt;/em&gt;&lt;/a&gt; if you're into that kind of thing.  But I will give the basic
ideas.  Imagine that you have a function &lt;span class="math"&gt;\(G\)&lt;/span&gt; that you are able to expand in a (complex) power series
that converges in a sense that it is simply not silly to act like it does:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \sum_{k=0}^{\infty} c_k z^n \lt \infty\\ 
\end{align}&lt;/div&gt;
&lt;p&gt;Suppose you also happen to have a recurrence relation for &lt;span class="math"&gt;\(c_j = f(\left\{c_i : i &amp;lt; j\right\})\)&lt;/span&gt;.
There is an arsenal not unlike that available for Fourier or Laplace transforms that allow you to
derive from that recurrence an analytic function &lt;span class="math"&gt;\(G(z)\)&lt;/span&gt;, which can be expanded into the form above.
Then, a &lt;em&gt;closed-form&lt;/em&gt; version of &lt;span class="math"&gt;\(c_j\)&lt;/span&gt; can read off from the &lt;span class="math"&gt;\(c_k\)&lt;/span&gt;'s in the series expansion of
&lt;span class="math"&gt;\(G(z)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If this is new to you, I'll say that it really helps to see it in action. But, we're not going to do
any of that here.  Instead, we are going to take some liberties and just use the machinery.  To
whit, the first thing we are going to do is realize that any series sum that involves powers of
things can be "converted" to a GF by taking some liberties and making some suitable replacements.&lt;/p&gt;
&lt;p&gt;Why would we want to do this?  Well, it all depends on how you choose to interpret the coefficients.
In particular, if you interpret the &lt;span class="math"&gt;\(c_k\)&lt;/span&gt; as &lt;em&gt;probabilities&lt;/em&gt;, then you actually get something that
is formally known as a probability generating function (PGF).&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \sum_{k=0}^{\infty} p_k z^k\\
\end{align}&lt;/div&gt;
&lt;p&gt;Notice a couple of things here: First, that &lt;span class="math"&gt;\(G(1) = 1\)&lt;/span&gt;.  Why? well 
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(1) &amp;amp;= \sum_{k=0}^{\infty} p_k 1^k\\
    &amp;amp;= \sum_{k=1}^{\infty} p_k = 1\\
\end{align}&lt;/div&gt;
&lt;p&gt;for everything that we consider to be a probability.  Also, notice that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G'(z) &amp;amp;= \sum_{k=1}^{\infty} k p_k z^{k-1}\\
G'(1) &amp;amp;= \sum_{k=1}^{\infty} k p_k = E[n].\\
\end{align}&lt;/div&gt;
&lt;p&gt;Now this is useful to us. Brute-force counting sequences to calculate &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt; could be replaced by
calculating the derivative of the PGF for the process.  If only we knew how to create the
PGF! Before I show you, let's observe one more fact about PGFs. Suppose we find ourselves in a
situation where we have a PGF that looks something like this&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \frac{P(z)}{F(z)}\,\\
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(P(z)\)&lt;/span&gt; and &lt;span class="math"&gt;\(F(z)\)&lt;/span&gt; are some analytic functions of &lt;span class="math"&gt;\(z\)&lt;/span&gt;. Observe that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
F'(z)\,G(z) + G'(z)\,F(z) &amp;amp;= P'(z)\\
\Rightarrow G'(1) &amp;amp;= \frac{P'(1) - F'(1)}{F(1)}\\
\end{align}&lt;/div&gt;
&lt;p&gt;If you have ever spent any time with partial fractions, you will know why we will want this result.
Now, let's use all of this on an unsuspecting sum to see if it works.  In light of thinking about
PGFs, our original &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; in terms of &lt;span class="math"&gt;\(T\)&lt;/span&gt; and &lt;span class="math"&gt;\(H\)&lt;/span&gt; is worth talking a look at.So, lets make the
replacements &lt;span class="math"&gt;\(H \rightarrow pz\)&lt;/span&gt; and &lt;span class="math"&gt;\(T \rightarrow qz\)&lt;/span&gt; and see what happens. This will allow us to
promote &lt;span class="math"&gt;\(S \rightarrow G(z)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S &amp;amp;= HT\left(\frac{1}{1-H} + \frac{1}{1-T}\right)\\
G(z) &amp;amp;= p q z^2 \left(\frac{1}{1-pz} + \frac{1}{1-qz}\right) &amp;amp;&amp;amp; S \rightarrow G(z)\,,\,H \rightarrow pz\,,\,T \rightarrow qz \\
&amp;amp;= \frac{p q z^2 \left(2-z\right)}{(1-pz)(1-qz)}\\
&amp;amp;= \frac{P(z)}{F(z)}\\
P'(1) &amp;amp;= p(1-p)\\
F'(1) &amp;amp;= 2p - 2p^2 -1\\
G'(1) &amp;amp;= \frac{p(1-p) - (2p -2p^2 - 1)}{p(1-p)}\\
&amp;amp;= \frac{p^2 - p + 1}{p(1-p)}.
\end{align}&lt;/div&gt;
&lt;p&gt;If you look back at our earlier calculation of &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt;, you'll see the same exact result.  So, we
accomplished nothing, except to convince ourselves that our machinery works.&lt;/p&gt;
&lt;h3&gt;The coin again, this time as a finite automaton&lt;/h3&gt;
&lt;p&gt;We return now to the hint that &lt;em&gt;Knuth et. al.&lt;/em&gt; gave us about using finite automata to help generate
our &lt;span class="math"&gt;\(S\)&lt;/span&gt;'s.  If we think about the coin problem we've been considering, a first stab at at
state machine could look something like this.&lt;/p&gt;
&lt;figure&gt;
&lt;a name="coin-automata"&gt;&lt;/a&gt; &lt;img src="images/coin-automata.svg"/&gt;

&lt;figcaption&gt;

Flipping a coin to see both sides as a state machine. States 1 and 3 mean that we've seen k T's or
H's respectively.

&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;I've labeled the states by number as well. We start in a state &lt;span class="math"&gt;\(0\)&lt;/span&gt; where we haven't seen a flip
yet. Then we start accumulating flips, moving to &lt;span class="math"&gt;\(1\)&lt;/span&gt; or &lt;span class="math"&gt;\(3\)&lt;/span&gt; initially after a &lt;span class="math"&gt;\(T\)&lt;/span&gt; or &lt;span class="math"&gt;\(H\)&lt;/span&gt;, then
eventually terminating when we get a flip different from the first flip we saw.  Now comes the
interesting part. Let's write down the sequence sum, like we did before, this time using the
subscript on &lt;span class="math"&gt;\(S\)&lt;/span&gt; not to index the number of trials, but to index the state &lt;span class="math"&gt;\(S_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_0 &amp;amp;= 1\\
S_1 &amp;amp;= TS_0 + TS_1 = T(1+S_1)\\
S_2 &amp;amp;= HS1\\
S_3 &amp;amp;= H(1+S3)\\
S_4 &amp;amp;= TS_3\\
S &amp;amp;= S_2 + S_4.\\
\end{align}&lt;/div&gt;
&lt;p&gt;Here we have identified &lt;span class="math"&gt;\(S_2\)&lt;/span&gt; and &lt;span class="math"&gt;\(S_4\)&lt;/span&gt; as our terminal states. Solving we see that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_2 &amp;amp;= \frac{TH}{1-T}\\
S_4 &amp;amp;= \frac{TH}{1-H}\\
S &amp;amp; = HT \left(\frac{1}{1-H} + \frac{1}{1-T}\right)\\
\end{align}&lt;/div&gt;
&lt;p&gt;This form should be familiar enough now that I don't have to convince you that this approach worked.&lt;/p&gt;
&lt;h3&gt;Happy Meals with Equal Probabilities&lt;/h3&gt;
&lt;p&gt;We are now prepared to take on the CCP for arbitrary &lt;span class="math"&gt;\(m\)&lt;/span&gt; with all &lt;span class="math"&gt;\(p_i = 1/m\)&lt;/span&gt;.  As before,
define a finite automata to describe the process.&lt;/p&gt;
&lt;figure&gt;
&lt;a name="m-automata"&gt;&lt;/a&gt; &lt;img src="images/m-automata.svg"/&gt;

&lt;figcaption&gt;

Gathering trinkets as a state machine. The state numbers count the number of distinct trinket's we
seen.

&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Here the states are labeled by the number of distinct trinkets we've seen. The symbol &lt;span class="math"&gt;\(X\)&lt;/span&gt; takes
the place of the &lt;span class="math"&gt;\(H\)&lt;/span&gt; or &lt;span class="math"&gt;\(T\)&lt;/span&gt; and represents the event of seeing a trinket. At the start, there are
&lt;span class="math"&gt;\(m\)&lt;/span&gt; ways we can move to state &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Once we are in state &lt;span class="math"&gt;\(1\)&lt;/span&gt;, there is one way we stay in that state,
and &lt;span class="math"&gt;\((m-1)\)&lt;/span&gt; ways we can move to state &lt;span class="math"&gt;\(2\)&lt;/span&gt;. And so on. Then we can read off &lt;span class="math"&gt;\(S\)&lt;/span&gt; as before:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_0 &amp;amp;= 1\\
S_1 &amp;amp;= m X S_0 + X S_1\\
&amp;amp;= \frac{m}{1-X}\,XS_0\\
S_2 &amp;amp;= \frac{(m - 1)}{1 - 2 X}\,XS_0\\
S_k &amp;amp;= \frac{(m - k + 1)}{1 - k X}\,XS_{k - 1}\\
&amp;amp;= f(X,m)_k\,XS_{k-1}\\
S_m &amp;amp;= XS_{m-1}.\\
\end{align}&lt;/div&gt;
&lt;p&gt;Here &lt;span class="math"&gt;\(f(X,m_k)\)&lt;/span&gt; has been introduced to hold the coefficient. Writing down a few terms for small &lt;span class="math"&gt;\(k\)&lt;/span&gt; of
the recurrence helps us see a closed form&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_4 &amp;amp;= X S_3 \\
    &amp;amp;= X^2 f(3)\,S_2 \\
    &amp;amp;= X^3 f(3)\,f(2)\,S_1 \\
    &amp;amp;= X^4 \prod_{k=1}^{3} f(k). \\
\end{align}&lt;/div&gt;
&lt;p&gt;This leads us to conclude that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_m &amp;amp;= X^m \prod_{k=1}^{m-1} \frac{m - k + 1}{1 - k X}\\
    &amp;amp;= m! X^m \prod_{k=1}^{m-1} \left(1 - k X \right)^{-1}.\\
\end{align}&lt;/div&gt;
&lt;p&gt;As before, we make the substitution &lt;span class="math"&gt;\( X \rightarrow pz = z/m\)&lt;/span&gt;, and we promote &lt;span class="math"&gt;\(S \rightarrow G(z)\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \frac{m!}{m^m} z^m \prod_{k=1}^{m-1} (1-kz/m)^{-1}.\\
\end{align}&lt;/div&gt;
&lt;p&gt;Also, as before, we make the substitution &lt;span class="math"&gt;\(G=z^{m}/F\)&lt;/span&gt; and recall that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
E[n] = G'(1) = \frac{m - F'(1)}{F(1)} \\
\end{align}&lt;/div&gt;
&lt;p&gt;Then
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
F(z) &amp;amp;= \frac{m^m}{m!} \prod_{k=1}^{m-1} (1- kz/m)\\
&amp;amp;= \frac{m^m}{m!} Q_m(z)\\
F'(z) &amp;amp;= \frac{-m^m}{m!} \frac{z}{m} Q_m(z) \sum_{k=1}^{m-1} \left(\frac{k}{1-kz/m}\right) \\
      &amp;amp;= -\frac{z}{m} F(z) \sum_{k=1}^{m-1} \left(\frac{k}{1-kz/m}\right) \\
      &amp;amp;= -z F(z) \sum_{k=1}^{m-1} \left(\frac{k}{m - kz}\right) \\
      &amp;amp;= z F(z) \sum_{k=1}^{m-1} \left(\frac{1}{z - m/k}\right) \\
\end{align}&lt;/div&gt;
&lt;p&gt;Things are looking very promising.  Now we evaluate &lt;span class="math"&gt;\(Q(1)\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
Q(1) &amp;amp;= \frac{1}{m^{m-1}} \prod_{k=1}^{m-1} \left(m-k\right) \ \\
     &amp;amp;= \frac{1}{m^{m-1}} \left( (m-1)(m-2) \ldots \left( m - (m - 1) \right) \right)\ \\
     &amp;amp;= \frac{\left(m-1\right)!}{m^{m-1}}\\
     &amp;amp;= \frac{m!}{m^m}.\\
\end{align}&lt;/div&gt;
&lt;p&gt;Interestingly, this means that &lt;span class="math"&gt;\(F(1) =1\)&lt;/span&gt;, which also means that &lt;span class="math"&gt;\(F\)&lt;/span&gt; also behaves like a PGF.  At any
rate, we were now left with&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
F'(1) &amp;amp;= \sum_{k=1}^{m-1} \frac{1}{1 - m/k}\\
\end{align}&lt;/div&gt;
&lt;p&gt;By now, we are tired and we resort to some &lt;a href="http://www.cs.cmu.edu/~odonnell/toolkit13/how-to-do-math-and-tcs.pdf"&gt;street fighting&lt;/a&gt;,
&lt;a href="https://www.wolframalpha.com/input/?i=sum+1%2F(1-m%2Fk),+k%3D1+to+m-1"&gt;Wolfram Alpha&lt;/a&gt;-style, which gives us&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
F'(1) &amp;amp;= m \left( 1 - \psi^{(0)} - \gamma\right) - 1\\
      &amp;amp;= m \left( 1 - (H_{m-1} - \gamma) - \gamma \right) - 1\\
      &amp;amp;= m \left( 1 - H_{m-1} \right) - 1\\
\end{align}&lt;/div&gt;
&lt;p&gt;using the fact that &lt;span class="math"&gt;\(\psi^{(0)} = H_{m-1} - \gamma\)&lt;/span&gt;. Of course, this fact is exactly what Alpha used
to give us the answer by re-arranging the sum.  Plugging this all in, we finally get&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G'(1) &amp;amp;= E[n]\\
      &amp;amp;= m - \left(m - m H_{m-1} - 1 \right)\\
      &amp;amp;= 1 + m H_{m-1}\\
      &amp;amp;= mH_m\\
\end{align}&lt;/div&gt;
&lt;p&gt;using &lt;span class="math"&gt;\(H_m = H_{m-1} + 1/m\)&lt;/span&gt;. This was our goal. &lt;/p&gt;
&lt;p&gt;Now to answer the original question for the equally probably Happy Meals.  In general we can get a
one-sided confidence interval for &lt;span class="math"&gt;\(n\)&lt;/span&gt;, the number of times we "roll" until success, by using &lt;span class="math"&gt;\(E[n]\)&lt;/span&gt;
and Markov's inequality.  If we could be bothered to calculate the variance using the formula's
above, we could use Chebyshev's to get a two sided confidence interval and a tighter bound. Anyway,&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
P(n &amp;gt; k m H_m) &amp;amp;\leq \frac{E[n]}{k m H_m}\\
&amp;amp;\leq \frac{1}{k}.\\
\end{align}&lt;/div&gt;
&lt;p&gt;Setting &lt;span class="math"&gt;\(1/k = 0.05 \Rightarrow k = 20\)&lt;/span&gt;. For example, for a six-sided die, for &lt;span class="math"&gt;\(\epsilon = kmH_m\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
P(n &amp;gt; \epsilon) &amp;amp;\leq 0.05\\
\epsilon &amp;amp;\approx 300 
\end{align}&lt;/div&gt;
&lt;p&gt;For a permutation of order &lt;span class="math"&gt;\(6\)&lt;/span&gt;, the 95% threshold is &lt;span class="math"&gt;\(\approx 200000\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, for large &lt;span class="math"&gt;\(m\)&lt;/span&gt;, &lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
H_m &amp;amp;\approx \ln{m} + \gamma\\
E[n] &amp;amp;\approx m\left( \ln{m} + \gamma \right)\\
\end{align}&lt;/div&gt;
&lt;p&gt;In my weird and twisted way, this is a much more satisfying approach to look at the Coupon
Collector Problem. Along the way, we practiced a technique that seems to be very powerful. For instance, I'm
interested to try my hand at using this technique on the &lt;a href="https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm"&gt;KMP Algorithm&lt;/a&gt;. I'm a fan of
&lt;a href="https://www.ics.uci.edu/~eppstein/"&gt;David Eppstein's Notes&lt;/a&gt; on algorithms from &lt;a href="https://www.ics.uci.edu/~eppstein/161/syl.html"&gt;long ago in internet time&lt;/a&gt;,
particularly where he shows a state machine for KMP:&lt;/p&gt;
&lt;figure&gt;
&lt;img src="https://www.ics.uci.edu/~eppstein/161/kmp.gif"/&gt;
&lt;figcaption&gt;

A state machine for matching "nano" with &lt;a
href="https://www.ics.uci.edu/~eppstein/161/960227.html"&gt;KMP&lt;/a&gt;.

&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Even though I don't like the canonical explanation, the Wikipedia page on CCP has many wonderful
links. In particular is one of my favorites by &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.5965&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;&lt;em&gt;Flajolet et. al.&lt;/em&gt;&lt;/a&gt;. That's a truly
systematic approach to the problem, and shows interesting connections to other allocation problems.&lt;/p&gt;
&lt;!-- &lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt; --&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=default";
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Generating Functions"></category><category term="Probability"></category><category term="Combinatorics"></category></entry><entry><title>Why you should know about the Gamma distribution</title><link href="http://memoizeme.com/blog/the-gamma-distribution.html" rel="alternate"></link><published>2016-12-02T00:00:00-08:00</published><updated>2016-12-02T00:00:00-08:00</updated><author><name>Soren Telfer</name></author><id>tag:memoizeme.com,2016-12-02:/blog/the-gamma-distribution.html</id><summary type="html">&lt;p&gt;I've been staring at exponentially distributed data in various forms for at least the last ten
years.  In my previous job, I spent a lot of time with the statistics of VoIP phone calls, trying to
understand what's "normal", and what are symptoms of system pathologies, or even the actions of
fraudsters in the telephony networks.  In approaching this subject long ago, I had not yet studied
queuing theory, but I had a lot of experience with Poisson processes in experimental high energy
physics. This meant that as I learned I had many wonderful eureka! moments.  One of these moments …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been staring at exponentially distributed data in various forms for at least the last ten
years.  In my previous job, I spent a lot of time with the statistics of VoIP phone calls, trying to
understand what's "normal", and what are symptoms of system pathologies, or even the actions of
fraudsters in the telephony networks.  In approaching this subject long ago, I had not yet studied
queuing theory, but I had a lot of experience with Poisson processes in experimental high energy
physics. This meant that as I learned I had many wonderful eureka! moments.  One of these moments
was when I learned that the sum of exponentially distributed variables has a gamma distribution. I
often find that people I mention this to, people in systems, and even often in machine learning, are
unfamiliar with this fact, or why it would be useful to know.&lt;/p&gt;
&lt;p&gt;In fact, this is a very helpful thing to know when looking at the distributions of many things you
measure in systems, for you often see a long-tailed distribution where the mode is shifted to the
right, relative to where the exponential would start. This is what &lt;code&gt;scipy.stats.gamma.rvs(1.99,
size=1000)&lt;/code&gt; looks like:&lt;/p&gt;
&lt;figure&gt;
&lt;img src="images/gamma.png" alt="A gamma"/&gt;
&lt;figcaption&gt;A gamma distribution from Scipy&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The reason this is really helpful to think about is you often find yourself looking at the length of
the intervals it takes for things to complete: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the time it takes IO requests to complete&lt;/li&gt;
&lt;li&gt;the time it takes a program to run&lt;/li&gt;
&lt;li&gt;the amount of time a lock is held&lt;/li&gt;
&lt;li&gt;the time it takes to resolve a ticket&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One way to think about what is happening behind the scenes is that you are looking at many
different system components taking part, and each taking some time to do it's job. What you see then
when you wait for the entire completion is then the sum of many individual component times. When we
model the individual times as exponentially distributed, we then should expect that what we measure
should follow a gamma-like distribution.  When reasoning about systems, looking at the distribution
of times tells you a lot about what's happening.  When the data looks exponentially-distributed,
i.e. the mode is all the way to the left (&lt;code&gt;scipy.stats.expon.rvs(size=1000)&lt;/code&gt;)&lt;/p&gt;
&lt;figure&gt;
&lt;img src="images/expon.png" alt="An Exponential"/&gt;
&lt;figcaption&gt;A Exponential distribution from Scipy&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;then you really have one dominant process contributing to the completion time. Instead, the more you
see the mode shifted to the right, the more you can reason that there are a number of processes of
equal contribution.  This is a useful way to reason about systems.&lt;/p&gt;
&lt;p&gt;As I said, learning that the sums of exponentials was distributed as gamma was a neat and useful
eureka! moment. However, until recently, I missed an extremely important related fact.  All of this
came about when I found a need to perform a computationally efficient test to determine if the
distributions two sets of exponentially distributed data differed. This lead to another eureka!
moment that I will describe below.&lt;/p&gt;
&lt;h3&gt;The Gamma distribution&lt;/h3&gt;
&lt;p&gt;Most folks are familiar with the gamma function, which extends the factorial function to complex numbers with positive real part:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\Gamma(z) &amp;amp;= \int_0^{\infty} t^{z-1} e^{-t} dt\\
&amp;amp;= (z-1)!,\, z \in \mathbb{C}, Re(z) &amp;gt; 0.\\
\end{align}&lt;/div&gt;
&lt;p&gt;But fewer people seem to know about the gamma distribution, which is defined to be
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(k, \theta) = \frac{1}{\Gamma(k)\,\theta^k} x^{k-1} e^{-x/\theta}\\
\end{align}&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(k\)&lt;/span&gt; is the scale and &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is the shape. The gamma distribution can be seen as a
generalization of two other commonly encountered distributions, &lt;span class="math"&gt;\(\chi_k^2\)&lt;/span&gt; and &lt;span class="math"&gt;\(Exp(\lambda)\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(k/2, 2) &amp;amp;= \frac{1}{\Gamma(k/2)\,2^{k/2}} x^{k/2-1} e^{-x/2}\\
&amp;amp;= \chi_k^2\\
G(1, \theta) &amp;amp;= \frac{1}{\theta} e^{-x/\theta}\\
&amp;amp;= Exp(\lambda = 1/\theta)\\
\end{align}&lt;/div&gt;
&lt;p&gt;The generality of the gamma goes even further. Not only is it is it's own conjugate prior, it's the
conjugate prior for a large number of important distributions: Poisson, Normal, Pareto, Log-normal,
Exponential.&lt;/p&gt;
&lt;p&gt;But let's get back to proving some neat things about this distribution. Before we can start, we will
need one additional piece of machinery.  In
&lt;a href="http://memoizeme.com/blog/coupon-collector.html"&gt;The Coupon Collector&lt;/a&gt;, I showed a useful application for
probability generating functions (PGFs), which, to review, are&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
G(z) &amp;amp;= \sum_0^\infty f_k z^k\\
&amp;amp;= \mathbb{E} \left[ z^k \right]\\
\end{align}&lt;/div&gt;
&lt;p&gt;under the discrete distribution &lt;span class="math"&gt;\(f_k\)&lt;/span&gt;.  A handy generalization of PGFs is create moment generating
functions (MGFs), by taking&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_X (t) &amp;amp;= G_X(e^t)\\
&amp;amp;= \mathbb{E}\left[ e^{tX}\right].\\
\end{align}&lt;/div&gt;
&lt;p&gt;MGFs have all the nice manipulation tricks of PGFs, have even nicer derivative-taking properties
than PGFs, and work for both continuous and discrete distributions.  And as long as we are careful
about integrating over the support of &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;, they also have the very interesting expression as&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_X (t) &amp;amp;= \int_{-\infty}^{\infty} e^{tx} f(x) dx\\
\end{align}&lt;/div&gt;
&lt;p&gt;which might be recognized as the two-sided Laplace transform of the distribution &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;.
 &lt;a href="http://memoizeme.com/blog/coupon-collector.html"&gt;Previously&lt;/a&gt;, we saw that one answer to "why did I do all of
these contortions on PGFs" was that taking derivatives gave us nice and easy ways to calculate the
mean, variance, etc. The MGFs also give us these as the moments of the distribution.  But the
definition above provides us another trick, namely that if we are careful about the the above
definition, we can take the inverse Laplace transform of a MGF to derive a distribution. Deriving
distributions can be trickery, so this is a nice tool to have in the box. We may return to
calculating &lt;span class="math"&gt;\(\mathcal{L}^{-1}\left\{ M_X(t) \right\}\)&lt;/span&gt; in another post.&lt;/p&gt;
&lt;p&gt;So, what is the MGF for the gamma distribution? By definition we have
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_{G(k,\theta)}(t) &amp;amp;= \mathbb{E} \left[ e^{tx} \right]\\
&amp;amp;= \int_0^\infty e^{tx} G\left(x \vert k,\theta\right) dx\\
&amp;amp;= \frac{1}{\Gamma(k)\, \theta^k} \int_0^\infty x^{k-1} e^{-x\left(1/\theta - t\right)} dx, \, t \theta &amp;lt; 1\\
&amp;amp;= \frac{1}{\Gamma(k)\, \theta^k\, (1/\theta - t)^k} \int_0^\infty x^{k-1} e^{-x} dx\\
&amp;amp;= \frac{1}{\Gamma(k)\, \theta^k\, (1/\theta - t)^k} \Gamma(k)\\
&amp;amp;= (1 - \theta t)^{-k}\\
\end{align}&lt;/div&gt;
&lt;p&gt;With that we are ready to derive the first result we need. Assume we have a set of IID random variables
&lt;span class="math"&gt;\(X_i \sim G(a_i, \theta)\)&lt;/span&gt; for some arbitrary &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; and fixed &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. Let's consider the sum of these variables
&lt;span class="math"&gt;\(S_n = \sum_n X_i\)&lt;/span&gt;. Now looking at the MGF, &lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
M_{S_n}(t) &amp;amp;= \mathbb{E}\left[ e^{t S_n} \right]\\
&amp;amp;= \mathbb{E}\left[ \prod_{i=1}^n e^{t x_i} \right]\\
&amp;amp;= \prod_{i=1}^n \mathbb{E}\left[e^{t x_i} \right]\\
&amp;amp;= \prod_{i=1}^n (1- \theta t)^{-a_i} \\
&amp;amp;= (1- \theta t)^{-\sum a_i} \\
&amp;amp;= M_{G(\sum a_i, \theta)}.\\
\end{align}&lt;/div&gt;
&lt;p&gt;In the third step we have used the independence of the &lt;span class="math"&gt;\(X_i\)&lt;/span&gt; to move the product outside of the
expectation. Looking at the last line, we identify the result as the MGF for &lt;span class="math"&gt;\(G(\sum a_i,
\theta)\)&lt;/span&gt;. We then conclude that&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
S_n &amp;amp;\sim G\left(\sum_{i=1}^n a_i, \theta\right).\\
\end{align}&lt;/div&gt;
&lt;p&gt;Which is to say that the sum of gamma distributed variables &lt;span class="math"&gt;\(X_i\)&lt;/span&gt; of scales &lt;span class="math"&gt;\(a_i\)&lt;/span&gt; is distributed as a gamma of scale &lt;span class="math"&gt;\(\sum a_i\)&lt;/span&gt;. This result carries my original eureka! moment, namely that if &lt;span class="math"&gt;\(X_i \sim Exp(\lambda = 1/\theta) =
G(1, \theta)\)&lt;/span&gt;, then &lt;/p&gt;
&lt;div class="math"&gt;$$ S_n = \sum_n X_i \sim G\left(n, \theta\right).  $$&lt;/div&gt;
&lt;p&gt;Incidentally this also means that if &lt;span class="math"&gt;\(X_i \sim \chi^2_{n_i} = G(n_i/2, 2)\)&lt;/span&gt;, then the sum over those variables&lt;/p&gt;
&lt;div class="math"&gt;$$ S_n = \sum_n X_i \sim G \left( 1/2 \sum n_i, 2\right) = \chi^2_{\sum n_i} $$&lt;/div&gt;
&lt;p&gt;A similar calculation to the above gives us another property of &lt;span class="math"&gt;\(G(k,\theta)\)&lt;/span&gt;, namely that if If &lt;span class="math"&gt;\(X \sim G(k, \theta)\)&lt;/span&gt;, then&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
cX &amp;amp;\sim G(k, c \theta).\\
\end{align}&lt;/div&gt;
&lt;p&gt;This means that if  &lt;span class="math"&gt;\(X_i \sim Exp(\lambda = 1/ \theta)\)&lt;/span&gt;,&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\frac{2}{\theta} S_n \sim G(n, 2) = \chi^2_{2n}.\\
\end{align}&lt;/div&gt;
&lt;p&gt;When I first saw this, I thought it was just amazing.  Now let me explain why.&lt;/p&gt;
&lt;h3&gt;Making sense of program run-times&lt;/h3&gt;
&lt;p&gt;As mentioned above, one of my current projects started all of this thinking.  I have been working on
a simple library to do performance-based regression testing, which I will talk more about that in a
later post, but for now, I present below some of the benchmarking data that I have been working
with. Each histogram is labeled by a number which is &lt;span class="math"&gt;\(k = log N\)&lt;/span&gt; of the total input size of the code
being tested. Specifically, I am benchmarking a library that uses Intel SSE4.2 specialized
instructions to calculate crc32c checksums.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="images/hists.png"/&gt;
&lt;figcaption&gt;Program runtimes for crc32c checksum calculation using Intel SSE4.2 instructions. Plots are labeled by input size.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are a ton of interesting details going on that I will discuss later, but it should be clear
that for each &lt;span class="math"&gt;\(k\)&lt;/span&gt;, every time I make these measurements I will get different looking
histograms. They could be different because something changed in the code. Or, they could be
different just because we are dealing with random variables. All I really want to know is if
something has changed in the code, assembly, or even underlying architecture. This will allow me to
fail the regression test with some meaningful statistical confidence. For profiling reasons, I might
be interested in the parameters, but primarily I need to detect changes.&lt;/p&gt;
&lt;p&gt;The usual way to deal with this kind of data situation is to take large number of samples &lt;span class="math"&gt;\(n_k\)&lt;/span&gt;. But
because my testing library needs to run very fast, taking a large number of sample is unacceptable
since the total test run time is proportional to the number of samples collected.  I need to test my
histogram sets against each other using as few total &lt;span class="math"&gt;\(\sum_k n_k\)&lt;/span&gt; as possible. Mainly this is
because in my experience, the probability of a test being manually disabled is proportional to the
time it takes to run the test. We need a system that no one every think about disabling.  Also, my
personal development style is to run the all dependent test-suites at every compile, not just at
every check-in. So, it needs to be fast.&lt;/p&gt;
&lt;p&gt;This is a good time to point out that by now most people looking at data like these have gone ahead
and assumed the values are Gaussian and have computed a &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; (and probably a &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;).  It is a
happy or sad (depending on your perspective) fact that the MLE for &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; for an exponential
distribution is&lt;/p&gt;
&lt;div class="math"&gt;$$ \hat{\lambda} = \frac{n}{\sum_i x_i} = \frac{1}{\bar{x}} $$&lt;/div&gt;
&lt;p&gt;For me it's sad because it lets people get away with taking averages of data like the above and
waving their hands around that they are quote-un-quote Gaussian. But I digress.&lt;/p&gt;
&lt;p&gt;For my own purposes, what I need is a meaningful statistical test that can help me detect a change
in the code.  This brings me to why I got excited when I learned that&lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{2}{\theta} S_n \sim G(n, 2) = \chi^2_{2n}. $$&lt;/div&gt;
&lt;p&gt;Seeing a &lt;span class="math"&gt;\(\chi^2\)&lt;/span&gt; gives me an idea. If &lt;span class="math"&gt;\(X_1 \sim \chi^2_{n_1}\)&lt;/span&gt; and &lt;span class="math"&gt;\(X_2 \sim \chi^2_{n_2}\)&lt;/span&gt;, then&lt;/p&gt;
&lt;div class="math"&gt;$$ U = \frac{X_1 / n_1}{X_2 / n_2} \sim F(2 n_1, 2 n_2) $$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(F\)&lt;/span&gt; is the &lt;span class="math"&gt;\(F\)&lt;/span&gt;-distribution.  In our case, &lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
U &amp;amp;= \frac{\frac{2 S_{n'}}{\theta' n'}}{\frac{2 S_n}{\theta n}}\\
&amp;amp;= \frac{\theta}{\theta'} \frac{\bar{x}'}{\bar{x}}\\
\end{align}&lt;/div&gt;
&lt;p&gt;is distributed in &lt;span class="math"&gt;\(F(2n',2n)\)&lt;/span&gt;.  Thus we test &lt;span class="math"&gt;\(H_0\)&lt;/span&gt; vs &lt;span class="math"&gt;\(H_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
H_0 &amp;amp;: \theta/\theta' = 1\\
H_1 &amp;amp;: \theta/\theta' \neq 1.\\
\end{align}&lt;/div&gt;
&lt;p&gt;Using tables or evaluation of &lt;span class="math"&gt;\(F\)&lt;/span&gt;, we can then derive a two sided &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value and also confidence
bounds. The &lt;span class="math"&gt;\(p\)&lt;/span&gt;-value will be found from&lt;/p&gt;
&lt;div class="math"&gt;$$
p = 2 P(F(2n',2n) &amp;lt; U)
$$&lt;/div&gt;
&lt;p&gt;whereas the confidence &lt;span class="math"&gt;\(95%\)&lt;/span&gt; interval is&lt;/p&gt;
&lt;div class="math"&gt;$$
P\left(\frac{U}{T_{0.975}} &amp;lt; \theta/\theta' &amp;lt; \frac{U}{T_{0.025}} \right) = 0.95
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(T_{0.975}\)&lt;/span&gt; and &lt;span class="math"&gt;\(T_{0.025}\)&lt;/span&gt; are the &lt;span class="math"&gt;\(m\)&lt;/span&gt; quantiles on &lt;span class="math"&gt;\(F(2n', 2n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;That's it for now. I'll explain how I'm using this in another post.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=default";
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Probability"></category><category term="Statistics"></category><category term="Generating Functions"></category></entry></feed>